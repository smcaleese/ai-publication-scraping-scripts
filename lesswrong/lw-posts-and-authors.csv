,titles,authors,dates
0,An Unexpected GPT-3 Decision in a Simple Gamble ,hatta_afiq,5m
1,"""Agency"" needs nuance",Evie Cottrell,9h
2,A type of CBT I find especially useful: ACT,Evie Cottrell,9h
3,Bathroom Construction Cost Comparison,jefftk,14h
4,Prioritizing the Arts in response to AI automation,Casey,14h
5,UI/UX From the Dark Ages,shminux,15h
6,First Principles for AI?,Chris Lengerich,16h
7,Everybody Comes Back,Alex Beyman,17h
8,Papers to start getting into NLP-focused alignment researchQ,Feraidoon,17h
9,AI coöperation is more possible than you think,423175,19h
10,"""Cotton Gin"" AI Risk",423175,19h
11,Two reasons we might be closer to solving alignment than it seems,"KatWoods, AmberDawn",21h
12,Orexin and the quest for more waking hours,ChristianKl,21h
13,Attempts at Forwarding Speed PriorsΩ,"james.lucassen, evhub",1d
14,"Announcing $5,000 bounty for ending malaria",lc,2d
15,Why Do AI researchers Rate the Probability of Doom So Low?Q,Aorou,2d
16,"Brain-over-body biases, and the embodied value problem in AI alignment",geoffreymiller,18h
17,Opt out from the Funni,Coafos,19h
18,AI Safety Discord community (requesting help!),Casey,1d
19,"I'm planning to start creating more write-ups summarizing my thoughts on various issues, mostly related to AI existential safety.  

What do you want to hear my nuanced takes on?Q",David Scott Krueger (formerly: capybaralet),1d
20,Set List Approaches,jefftk,2d
21,A ranked link of LessWrong tags/concepts,peterslattery,2d
22,Posts with clickable sections of images?Q,NoBadCake,2d
23,Under what circumstances have governments cancelled AI-type systems?,David Gross,2d
24,There are no rules,unoptimal,2d
25,Interpreting Neural Networks through the Polytope LensΩ,"Sid Black, Lee Sharkey, Connor Leahy, beren, CRG, merizian, EricWinsor, Dan Braun",2d
26,The heterogeneity of human value types: Implications for AI alignment,geoffreymiller,2d
27,How to use DMT without going insane: On navigating epistemic uncertainty in the DMT memeplex,cube_flipper,2d
28,Shahar Avin On How To Regulate Advanced AI Systems,Michaël Trazzi,2d
29,Interlude: But Who Optimizes The Optimizer?,Paul Bricman,2d
30,Why do som many things break in a 2 element set?,OldManNick,2d
31,Intelligence as a Platform,Robert Kennedy,2d
32,"Public-facing Censorship Is Safety Theater, Causing Reputational Damage ",Yitz,2d
33,A game of mattering,KatjaGrace,3d
34,Making Prunes,jefftk,3d
35,Funding is All You Need: Getting into Grad School by Hacking the NSF GRFP Fellowship,hapanin,3d
36,What Do AI Safety Pitches Not Get About Your Field?Q,Aris,3d
37,"""Free Will"" in a Computational Universe",DragonGod,3d
38,"Initial Thoughts on Dissolving ""Couldness""",DragonGod,3d
39,Let's Compare Notes,Shos Tekofsky,3d
40,Methodological Therapy: An Agenda For Tackling Research BottlenecksΩ,"adamShimi, Lucas Teixeira, remember",3d
41,Fake qualities of mind,Kaj_Sotala,3d
42,Dath Ilan's Views on Stopgap Corrigibility,David Udell,3d
43,AI Risk Intro 2: Solving The Problem,"TheMcDouglas, LRudL",3d
44,Understanding Infra-Bayesianism: A Beginner-Friendly Video Series,"Jack Parker, Connall Garrod",3d
45,Is the game design/art maxim more generalizable to criticism/praise itself?Q,Noosphere89,3d
46,Mathematical Circuits in Neural Networks,Sean Osier,4d
47,"Berkeley group house, spots open",Jack R,3d
48,Ukraine Post #12,Zvi,3d
49,Covid 9/22/22: The Joe Biden Sings,Zvi,3d
50,AI careerQ,ondragon,4d
51,Here Be AGI Dragons,Oren Montano,4d
52,An issue with MacAskill's Evidentialist's Wager,Martín Soto,4d
53,"Nearcast-based ""deployment problem"" analysisΩ",HoldenKarnofsky,4d
54,Scraping training data for your mind,Henrik Karlsson,4d
55,Trends in Training Dataset Sizes,Pablo Villalobos,4d
56,"Can you define ""utility"" in utilitarianism without using words for specific human emotions?Q",SurvivalBias,5d
57,"""Infohazards"" The ML Field's Greatest Excuse.",Puffy Bird,5d
58,Case Rates to Sequencing Reads,jefftk,5d
59,Towards deconfusing wireheading and reward maximizationΩ,leogao,5d
60,What key nutrients are required for daily energy?Q,Trevor1,5d
61,"Announcing AISIC 2022 - the AI Safety Israel Conference, October 19-20Ω",Davidmanheim,4d
62,Quantified Intuitions: An epistemics training website including a new EA-themed calibration app,"Sage Future, elifland",5d
63,The Redaction Machine,Ben,5d
64,You Are Not Measuring What You Think You Are Measuring,johnswentworth,1d
65,What happened to the idea of progress?,jasoncrawford,5d
66,Features and Antifeatures,Davis_Kingsley,5d
67,Cryptocurrency Exploits Show the Importance of Proactive Policies for AI X-Risk,eSpencer,5d
68,Alignment Org Cheat Sheet,"Akash, Thomas Larsen",5d
69,Doing oversight from the very start of training seems hardΩ,peterbarnett,5d
70,"$13,000 of prizes for changing our mind about who to fund (Clearer Thinking Regrants Forecasting Tournament)",spencerg,5d
71,"Progress links and tweets, 2022-09-20",jasoncrawford,5d
72,"If we have Human-level chatbots, won't we end up being ruled by possible people?Q",Erlja Jkdf.,5d
73,Twitter Polls: Evidence is Evidence,Zvi,5d
74,Some of the most important entrepreneurship skills are tacit knowledge,Ruhul,5d
75,Character alignment,p.b.,5d
76,Losing the root for the tree,Adam Zerner,6d
77,How to make your CPU as fast as a GPU - Advances in Sparsity w/ Nir Shavit,the gears to ascenscion,6d
78,Do bamboos set themselves on fire?,Malmesbury,6d
79,Quintin's alignment papers roundup - week 2,Quintin Pope,6d
80,Some notes on solving hard problems,Joe Rocca,6d
81,Safety timelines: How long will it take to solve alignment?,"Esben Kran, JonathanRystroem, Steinthal, Apart Research",6d
82,The ELK Framing I’ve Used,sudo -i,6d
83,Quick Book Review: Crucial Conversations,G Gordon Worley III,6d
84,How my team at Lightcone sometimes gets stuff done,jacobjacob,6d
85,EA & LW Forums Weekly Summary (12 - 18 Sep '22),Zoe Williams,6d
86,Prize idea: Transmit MIRI and Eliezer's worldviews,elifland,6d
87,Rationality Dojo Berlin Handout,UnplannedCauliflower,6d
88,A noob goes to the SERI MATS presentations,Lowell Dennings,6d
89,PIBBSS (AI alignment) is hiring for a Project ManagerΩ,Nora_Ammann,6d
90,Book Swap,Screwtape,7d
91,Pretending not to Notice,jefftk,7d
92,"[To Be Revised]Perhaps the Meaning of Life, An Adventure in Pluralistic Morality",NoBadCake,7d
93,Leveraging Legal Informatics to Align AI,John Nay,7d
94,The Inter-Agent Facet of AI AlignmentΩ,Michael Oesterle,7d
95,Summaries: Alignment Fundamentals CurriculumΩ,Leon Lang,7d
96,Inner alignment: what are we pointing at?Ω,lcmgcd,7d
97,"Podcasts on surveys, slower AI, AI arguments, etc",KatjaGrace,7d
98,There is no royal road to alignment,Eleni Angelou,8d
99,Biden should be applauded for appointing Renee Wegrzyn for ARPA-H,ChristianKl,7d
100,Updates on FLI's Value Aligment Map?Q,rodeo_flagellum,8d
101,Sparse trinary weighted RNNs as a path to better language model interpretability,Am8ryllis,8d
102,Refine's Third Blog Post Day/WeekΩ,adamShimi,8d
103,Many therapy schools work with inner multiplicity (not just IFS),"David Althaus, Ewelina Tur",8d
104,"Should AI learn human values, human norms or something else?",Q Home,8d
105,Takeaways from our robust injury classifier project [Redwood Research]Ω,DMZ,9d
106,Emergency Residential Solar Jury-Rigging,jefftk,9d
107,A Bite Sized Introduction to ELK,Luk27182,9d
108,D&D.Sci September 2022: The Allocation Helm,abstractapplic,9d
109,Most sensible abstraction & feature set for a systems language?,Jasen Qin,8d
110,Apply for mentorship in AI Safety field-building,Akash,8d
111,Prize and fast track to alignment research at ALTERΩ,Vanessa Kosoy,8d
112,Remote Login For Turnkey Devices?,jefftk,8d
113,Why doesn't China (or didn't anyone) encourage/mandate elastomeric respirators to control COVID?Q,Wei_Dai,9d
114,Towards a philosophy of safety,jasoncrawford,9d
115,Refine Blogpost Day #3: The shortforms I did writeΩ,Self-Embedded Agent,9d
116,"Why are we sure that AI will ""want"" something?Q",shminux,9d
117,"Katja Grace on Slowing Down AI, AI Expert Surveys And Estimating AI Risk",Michaël Trazzi,9d
118,Levels of goals and alignmentΩ,zeshen,9d
119,ordering capability thresholdsΩ,carado,9d
120,Representational Tethers: Tying AI Latents To Human OnesΩ,Paul Bricman,9d
121,Guidelines for Mad Entrepreneurs,David Udell,9d
122,I wrote a fantasy novel to promote EA: More Chapters,Timothy Underwood,9d
123,Affordable Housing Investment Fund,jefftk,10d
124,A market is a neural network,David Hugh-Jones,10d
125,Understanding Conjecture: Notes from Connor Leahy interview,Akash,10d
126,How should DeepMind's Chinchilla revise our AI forecasts?,strawberry calm,10d
127,Are Human Brains Universal?Q,DragonGod,10d
128,Intelligence failures and a theory of change for forecasting,NathanBarnard,10d
129,AGI safety researchers should focus (only/mostly) on deceptive alignmentΩ,Marius Hobbhahn,10d
130,Logical counterfactuals are brittle; use conditionals,Sylvester Kollin,10d
131,What's the longest a sentient observer could survive in the Dark Era?Q,"Raemon, Diffractor",10d
132,The Value of Not Being an Imposter,sudo -i,10d
133,Capability and Agency as Cornerstones of AI risk ­— My current model,wilm,10d
134,General advice for transitioning into Theoretical AI Safety,Martín Soto,10d
135,Sequencing Intro II: Adapters,jefftk,11d
136,How do I find tutors for obscure skills/subjects (i.e. fermi estimation tutors)Q,joraine,11d
137,"AstralCodexTen and Rationality 
Meetup Organisers’ Retreat —
Europe, Middle East, and Africa 2023",Sam,10d
138,Rational Animations' Script Writing Contest,Writer,10d
139,Covid 9/15/22: Permanent Normal,Zvi,10d
140,Effective altruism in the garden of ends,Tyler Alterman,11d
141,The problem with the media presentation of “believing in AI”,Roman Leventov,11d
142,Seeing the Schema,vitaliya,11d
143,Responding to 'Beyond Hyperanthropomorphism',ukc10014,11d
144,When is intent alignment sufficient or necessary to reduce AGI conflict? Ω,"JesseClifton, Sammy Martin, antimonyanthony",11d
145,When would AGIs engage in conflict?Ω,"JesseClifton, Sammy Martin, antimonyanthony",11d
146,When does technical work to reduce AGI conflict make a difference?: IntroductionΩ,"JesseClifton, Sammy Martin, antimonyanthony",11d
147,ACT-1: Transformer for Actions,Daniel Kokotajlo,11d
148,Renormalization: Why Bigger is Simpler,tailcalled,11d
149,Guesstimate Algorithm for Medical Research,Elizabeth,11d
150,Precise P(doom) isn't very important for prioritization or strategy,harsimony,11d
151,"Transhumanism, genetic engineering, and the biological basis of intelligence.",fowlertm,11d
152,What would happen if we abolished the FDA tomorrow?,Yair Halberstadt,11d
153,Emily Brontë on: Psychology Required for Serious™ AGI Safety Research,robertzk,11d
154,The Defender’s Advantage of InterpretabilityΩ,Marius Hobbhahn,11d
155,Why Do People Think Humans Are Stupid?Q,"DragonGod, tailcalled",11d
156,Trying to find the underlying structure of computational systemsΩ,Matthias G. Mayer,12d
157,Risk aversion and GPT-3,hatta_afiq,12d
158,Simple proofs of the age of the universe (or other things),Astynax,12d
159,An investigation into when agents may be incentivized to manipulate our beliefs.,Felix Hofstätter,12d
160,Deep Q-Networks Explained,Jay Bailey,12d
161,Ideas of the Gaps,Q Home,12d
162,Which LessWrong content would you like recorded into audio/podcast form?Q,Ruby,13d
163,EA & LW Forums Weekly Summary (5 - 11 Sep 22'),Zoe Williams,13d
164,"New tool for exploring EA Forum, LessWrong and Alignment Forum - Tree of TagsΩ",Filip Sondej,12d
165,How To Actually Succeed,Jordan Arel,13d
166,Time is not the bottleneck (on making progress thinking about difficult things),kman,13d
167,[Linkpost] A survey on over 300 works about interpretability in deep networksΩ,scasper,13d
168,Contemporary Linguistics: A Perspective on Research and Information Sharing,Miniman,13d
169,"Why do People Think Intelligence Will be ""Easy""?Q",DragonGod,13d
170,Alignment via prosocial brain algorithms,Cameron Berg,13d
171,I’ve written a Fantasy Novel to Promote Effective Altruism,Timothy Underwood,13d
172,Ideological Inference Engines: Making Deontology Differentiable*Ω,Paul Bricman,13d
173,Can you force a neural network to keep generalizing?,Q Home,13d
174,Argument against 20% GDP growth from AI within 10 years [Linkpost],aogara,14d
175,AI Safety field-building projects I'd like to see,Akash,14d
176,Freeloading?,jefftk,13d
177,Black Box Investigation Research Hackathon,"Esben Kran, Jonas Hallgren, Apart Research",13d
178,Fermi Paradox: Iron Age Milky Way,Rofel Wodring,14d
179,You Don't Have To Click The Links,Simon Berens,14d
180,The Ultimate Step-by-Step Hiring Playbook,intellectronica,14d
181,"In forecasting, how do accuracy, calibration and reliability relate to each other?Q",amarai,14d
182,Briefly thinking through some analogs of debate,Eli Tyre,14d
183,AI Risk Intro 1: Advanced AI Might Be Very Bad,"TheMcDouglas, LRudL",14d
184,A Pin and a Balloon: Anthropic Fragility Increases Chances of Runaway Global Warming ,avturchin,14d
185,Is there an Ultimate text editor? Q,Johannes C. Mayer,14d
186,Improving Human Evaluation of Factual Accuracy in Language Models,Soren,15d
187,Making a New Table Leaf,jefftk,14d
188,Introducing School of Thinking,Luca Parodi,14d
189,Unbounded utility functions and precommitment,MichaelStJules,15d
190,"What is the ""Less Wrong"" approved acronym for 1984-risk?Q","Logan Zoellner, Yoav Ravid",15d
191,Find out how utilitarian you are - a mega thread of philosophy polls,spencerg,15d
192,Quintin's alignment papers roundup - week 1Ω,Quintin Pope,15d
193,Path dependence in ML inductive biasesΩ,"Vivek Hebbar, evhub",16d
194,Keeping Time in Epoch Seconds,G Gordon Worley III,16d
195,"Pascal: The Greatness and Littleness of Man, A Thinking Reed",NoBadCake,15d
196,[Job] Project Manager: Community Health (CEA),Xodarap,15d
197,Put Dirty Dishes in the Dishwasher,jefftk,15d
198,Join ASAP! (AI Safety Accountability Programme) 🚀,TheMcDouglas,15d
199,Ought will host a factored cognition “Lab Meeting”Ω,"jungofthewon, stuhlmueller",16d
200,Web4/Heaven - The Simulation,Dunning K.,16d
201,Swap and Scale,Stephen Fowler,16d
202,AlexaTM - 20 Billion Parameter Model With Impressive Performance,ViktorThink,16d
203,Gatekeeper Victory: AI Box Reflection,"Double, DaemonicSigil",16d
204,AI alignment with humans... but with which humans?,geoffreymiller,16d
205,Notes on Resolve,David Gross,16d
206,ethics and anthropics of homomorphically encrypted computations,carado,16d
207,Oversight Leagues: The Training Game as a FeatureΩ,Paul Bricman,16d
208,Understanding and avoiding value driftΩ,TurnTrout,17d
209,Samotsvety's AI risk forecasts,elifland,17d
210,Evaluations project @ ARC is hiring a researcher and a webdev/engineerΩ,Beth Barnes,16d
211,My emotional reaction to the current funding situation,Sam,16d
212,[Fun][Link] Alignment SMBC Comic,Gunnar_Zarncke,16d
213,Interpreting Affordable Housing,jefftk,16d
214,London Rationalish Meetup 2022-09-11,calmiguana,16d
215,Should you refrain from having children because of the risk posed by artificial intelligence?Q,Mientras,16d
216,[An email with a bunch of links I sent an experienced ML researcher interested in learning about Alignment / x-safety.]Ω,David Scott Krueger (formerly: capybaralet),17d
217,"Progress links & tweets, 2022-09-08",jasoncrawford,17d
218,Turning WhatsApp Chat Data into Prompt-Response Form for Fine-Tuning,hatta_afiq,17d
219,Thoughts on AGI consciousness / sentience,Steven Byrnes,17d
220,A rough idea for solving ELK: An approach for training generalist agents like GATO to make plans and describe them to humans clearly and honestly.,Michael Soareverix,17d
221,"What Should AI Owe To Us?
Accountable and Aligned AI Systems via Contractualist AI AlignmentΩ",xuan,17d
222,Solar Blackout Resistance,jefftk,17d
223,All AGI safety questions welcome (especially basic ones) [~monthly thread],plex,17d
224,Sequences/Eliezer essays beyond those in AI to Zombies?Q,Domenic,17d
225,Linkpost: Github Copilot productivity experimentΩ,Daniel Kokotajlo,18d
226,Searching for Modularity in Large Language Models,"NickyP, Stephen Fowler",18d
227,90% of anything should be bad (& the precision-recall tradeoff),cartografie,18d
228,How to Do Research. v1,Pablo Repetto,18d
229,Postmortem: Trying out for Manifold Markets,"Milli, Austin Chen",17d
230,Covid 9/8/22: Booster Boosting,Zvi,17d
231,OpenPrinciples Bootcamp (Free) -- Reflect & Act on your Rationality Principles.,ti_guo,18d
232,"In a lack of data, how should you weigh credences in theoretical physics's Theories of Everything, or TOEs?Q",Noosphere89,18d
233,Generators Of Disagreement With AI Alignment,George3d6,18d
234,Shrödinger’s lottery or: Why you are going to live forever,Chase Dowdell,18d
235,Is training data going to be diluted by AI-generated content?,Hannes Thurnherr,18d
236,It's (not) how you use it ,Eleni Angelou,18d
237,First we shape our social graph; then it shapes us,Henrik Karlsson,18d
238,AI-assisted list of ten concrete alignment things to do right nowΩ,lcmgcd,18d
239,"Can ""Reward Economics"" solve AI Alignment?",Q Home,18d
240,Is there a list of projects to get started with Interpretability?,Franziska Fischer,19d
241,Progress Report 7: making GPT go hurrdurr instead of brrrrrrr,Nathan Helm-Burger,19d
242,Framing AI ChildhoodsΩ,David Udell,19d
243,Galaxy Trucker Needs a New Second Half,jefftk,18d
244,Rejected Early Drafts of Newcomb's Problem,zahmahkibo,19d
245,How can we secure more research positions at our universities for x-risk researchers?Q,Neil Crawford,19d
246,Community Building for Graduate Students: A Targeted Approach,Neil Crawford,19d
247,How Josiah became an AI safety researcher,Neil Crawford,19d
248,"No, human brains are not (much) more efficient than computers",jhoogland,19d
249,On oxytocin-sensitive neurons in auditory cortex,Steven Byrnes,19d
250,EA & LW Forums Weekly Summary (28 Aug - 3 Sep 22’),Zoe Williams,19d
251,Alex Lawsen On Forecasting AI Progress,Michaël Trazzi,19d
252,What are you for?,lsusr,20d
253,The Power (and limits?) of Chunking,NicholasKross,20d
254,Deleted comments archive,Said Achmiz,19d
255,Guitar Pedals on Fiddle,jefftk,19d
256,Another Unphrased B-part,jefftk,20d
257,[Exploratory] Becoming more Agentic,Johannes C. Mayer,20d
258,AI Governance Needs Technical Work,Mauricio,20d
259,program searches,carado,20d
260,Overton Gymnastics: An Exercise in Discomfort,"Shos Tekofsky, omark",20d
261,Beta Readers are Great,HoldenKarnofsky,20d
262,Impact Shares For Speculative Projects,Elizabeth,20d
263,"An unofficial ""Highlights from the Sequences"" tier list",Akash,20d
264,(Link) I'm Missing a Chunk of My Brain,mukashi,21d
265,The Good King,GregorDeVillain,20d
266,A Game About AI Alignment (& Meta-Ethics): What Are the Must Haves?,JonathanErhardt,20d
267,[Exploratory] What does it mean that an experiment is high bit?,Johannes C. Mayer,21d
268,"EA, Veganism and Negative Animal Utilitarianism",Yair Halberstadt,21d
269,The ethics of reclining airplane seats,braces,21d
270,Turn your flashcards into Art,Heye Groß,21d
271,Let's Terraform West Texas,blackstampede,21d
272,Help me find a good Hackathon subject Q,Raphaël S,21d
273,The shard theory of human valuesΩ,"Quintin Pope, TurnTrout",22d
274,Breaking Newcomb's Problem with Non-Halting states,Slimepriestess,22d
275,Monthly Shorts 8/22,Celer,22d
276,How To Know What the AI Knows - An ELK Distillation,Fabien,22d
277,Private alignment research sharing and coordination,porby,22d
278,AXRP Episode 18 - Concept Extrapolation with Stuart ArmstrongΩ,DanielFilan,22d
279,What's your Mission?,GregorDeVillain,21d
280,Russian Food for Petrov Day,weft,21d
281,Prototyping in C,jefftk,21d
282,Bay Solstice 2022 Call For Volunteers,Scott Alexander,21d
283,Fully Live Electronic Contra,jefftk,22d
284,An Update on Academia vs. Industry (one year into my faculty job)Ω,David Scott Krueger (formerly: capybaralet),22d
285,Request for Alignment Research Project RecommendationsQ,Rauno Arike,22d
286,Three scenarios of pseudo-alignment ,Eleni Angelou,22d
287,Bugs or Features?,qbolec,22d
288,[Exploratory] Seperate exploratory writing from public writing,Johannes C. Mayer,23d
289,We may be able to see sharp left turns comingΩ,"Ethan Perez, Neel Nanda",23d
290,Can someone explain to me why most researchers think alignment is probably something that is humanly tractable?Q,"iamthouthouarti, Quintin Pope",23d
291,Behaviour Manifolds and the Hessian of the Total Loss - Notes and CriticismΩ,Spencer Becker-Kahn,23d
292,[Exploratory] Exploratory Writing Info,Johannes C. Mayer,23d
293,Sticky goals: a concrete experiment for understanding deceptive alignmentΩ,evhub,23d
294,"Agency engineering: is AI-alignment ""to human intent"" enough?",catubc,23d
295,Laziness in AI,Richard Henage,23d
296,SimulatorsΩ,janus,21d
297,Levelling Up in AI Safety Research Engineering,Gabriel Mukobi,23d
298,Appendix: How to run a successful Hamming circle,CFAR!Duncan,24d
299,Replacement for PONR conceptΩ,Daniel Kokotajlo,24d
300,AI coordination needs clear winsΩ,evhub,24d
301,Exporting Hangouts History,jefftk,23d
302,Stop Discouraging Microwave Formula Preparation,jefftk,24d
303,A Richly Interactive AGI Alignment Chart,lisperati,24d
304,Short story speculating on possible ramifications of AI on the art world,Yitz,24d
305,Why was progress so slow in the past?,jasoncrawford,24d
306,"AI Safety and Neighboring Communities: A Quick-Start Guide, as of Summer 2022Ω",Sam Bowman,24d
307,Gradient Hacker Design Principles From BiologyΩ,johnswentworth,24d
308,Book review: Put Your Ass Where Your Heart Wants to Be,Ruhul,24d
309,A Survey of Foundational Methods in Inverse Reinforcement Learning,adamk,24d
310,I Tripped and Became GPT! (And How This Updated My Timelines),Frankophone,24d
311,"Alignment is hard. Communicating that, might be harder ",Eleni Angelou,24d
312,A Starter-kit for Rationality Space,jhoogland,24d
313,Pondering the paucity of volcanic profanity post Pompeii perusal,CraigMichael,24d
314,"Infra-Exercises, Part 1Ω","Diffractor, Jack Parker, Connall Garrod",24d
315,Strategy For Conditioning Generative ModelsΩ,"james.lucassen, evhub",25d
316,Safety Committee Resources,jefftk,25d
317,"Fixed point theory (locally (α,β,ψ) dominated contractive condition)Q",muzammil,24d
318,Covid 9/1/22: Meet the New Booster,Zvi,24d
319,"Progress links and tweets, 2022-08-31",jasoncrawford,25d
320,Enantiodromia,ChristianKl,25d
321,"New 80,000 Hours problem profile on existential risks from AI",Benjamin Hilton,25d
322,How much impact can any one man have?,GregorDeVillain,25d
323,How might we make better use of AI capabilities research for alignment purposes?Q,ghostwheel,1mo
324,AI Box Experiment: Are people still interested?Q,"Double, Nathan Helm-Burger",1mo
325,Survey of NLP Researchers: NLP is contributing to AGI progress; major catastrophe plausibleΩ,Sam Bowman,1mo
326,"Supposing Europe is headed for a serious energy crisis this winter, what can/should one do as an individual to prepare?Q",Erich_Grunewald,25d
327,Grand Theft Education,Zvi,25d
328,ProjectLawful.com gives you policy experience,Trevor1,1mo
329,Worlds Where Iterative Design FailsΩ,johnswentworth,1mo
330,Inner Alignment via Superpowers,"JamesH, Thomas Larsen, Jeremy Gillen",1mo
331,ML Model Attribution Challenge [Linkpost],aogara,1mo
332,How likely is deceptive alignment?Ω,evhub,1mo
333,A bayesian updating on expert opinionsQ,amarai,1mo
334,Any Utilitarianism Makes Sense As Policy ,George3d6,1mo
335,"A gentle primer on caring, including in strange senses, with applications",kh,1mo
336,Modified Guess Culture,konstell,1mo
337,What is the best critique of AI existential risk arguments?Q,Josh,1mo
338,How to plan for a radically uncertain future?,Kerry,1mo
339,EA & LW Forums Weekly Summary (21 Aug - 27 Aug 22'),Zoe Williams,1mo
340,Can We Align a Self-Improving AGI?,Peter S. Park,1mo
341,Built-In Bundling For Faster Loading,jefftk,1mo
342,On the nature of help - a framework for helping,Faustify,1mo
343,Fundamental Uncertainty: Chapter 4 - Why don't we do what we think we should?,G Gordon Worley III,1mo
344,How can I reconcile the two most likely requirements for humanities near-term survival.Q,Erlja Jkdf.,1mo
345,LessWrong's prediction on apocalypse due to AGI (Aug 2022),LetUsTalk,1mo
346,Are Generative World Models a Mesa-Optimization Risk?,Thane Ruthenis,1mo
347,Sequencing Intro,jefftk,1mo
348,How Do AI Timelines Affect Existential Risk?,Stephen McAleese,1mo
349,How might we align transformative AI if it’s developed very soon?Ω,HoldenKarnofsky,1mo
350,Please Do Fight the Hypothetical,Conor Sullivan,1mo
351,Have you considered getting rid of death?,Willa,1mo
352,(My understanding of) What Everyone in Technical Alignment is Doing and WhyΩ,"Thomas Larsen, elifland",12d
353,*New* Canada AI Safety & Governance communityΩ,Wyatt Tessari L'Allié,1mo
354,An Audio Introduction to Nick Bostrom,PeterH,1mo
355,The AGI has to actually 'care' about humans,Josh,1mo
356,Breaking down the training/deployment dichotomy,Erik Jenner,1mo
357,More Clothes Over Time?,jefftk,1mo
358,The Expanding Moral Cinematic Universe,Raemon,1mo
359,An Introduction to Current Theories of Consciousness,hohenheim,1mo
360,[Linkpost] Can lab-grown brains become conscious?,Jack R,1mo
361,Robert Long On Why Artificial Sentience Might Matter,Michaël Trazzi,1mo
362,Artificial Moral Advisors: A New Perspective from Moral Psychology,David Gross,1mo
363,Pronunciations,Solenoid_Entity,1mo
364,First thing AI will do when it takes over is get fission going,visiax,1mo
365,Who ordered alignment's apple? ,Eleni Angelou,1mo
366,Sufficiently many Godzillas as an alignment strategy,142857,1mo
367,What would you expect a massive multimodal online federated learner to be capable of?Q,Aryeh Englander,1mo
368,Basin broadness depends on the size and number of orthogonal featuresΩ,"TheMcDouglas, Avery, Lucius Bushnaq",1mo
369,"Solving Alignment by ""solving"" semantics",Q Home,1mo
370,Help Understanding Preferences And Evil,Netcentrica,1mo
371,Is there a benefit in low capability AI Alignment research?,Letti,1mo
372,Contra Dance Contact Tracing,jefftk,1mo
373,Annual AGI Benchmarking EventΩ,Lawrence Phillips,1mo
374,Taking the parameters which seem to matter and rotating them until they don't,Garrett Baker,1mo
375,ACX Meetups Everywhere List,Scott Alexander,1mo
376,What's the Most Impressive Thing That GPT-4 Could Plausibly Do?,bayesed,1mo
377,Is population collapse due to low birth rates a problem?Q,mukashi,1mo
378,Double Crux In A Box,Screwtape,1mo
379,Seeking Student Submissions: Edit Your Source Code Contest,Aris,1mo
380,AI Risk in Terms of Unstable Nuclear Software,Thane Ruthenis,1mo
381,Could you please share a tool to help with reasoning or make better decisions?Q,hodovani,1mo
382,Some conceptual alignment research projectsΩ,Richard_Ngo,24d
383,Variational Bayesian methods,Ege Erdil,1mo
384,A Test for Language Model ConsciousnessΩ,Ethan Perez,1mo
385,AI strategy nearcastingΩ,HoldenKarnofsky,1mo
386,Evaluating OpenAI's alignment plans using training stories,ojorgensen,1mo
387,Common misconceptions about OpenAIΩ,Jacob_Hilton,1mo
388,Your posts should be on arXivΩ,JanBrauner,1mo
389,The Solomonoff prior is malign. It's not a big deal.,Charlie Steiner,1mo
390,[Review] The Problem of Political Authority by Michael Huemer,Arjun Panickssery,1mo
391,The Shard Theory Alignment Scheme,David Udell,1mo
392,Preparing for the apocalypse might help prevent it,Ocracoke,1mo
393,"Event in SF: Foresight Institute meetup, Sep 8",jasoncrawford,1mo
394,Covid 8/25/22: What We Owe,Zvi,1mo
395,What Makes A Good Measurement Device?Ω,johnswentworth,1mo
396,Consciousness Actually Explained: EC Theory,Zahima,1mo
397,Google AI integrates PaLM with robotics: SayCan update [Linkpost]Ω,Evan R. Murphy,1mo
398,Vingean AgencyΩ,abramdemski,1mo
399,OpenAI's Alignment Plans,dkirmani,1mo
400,Beliefs and Disagreements about Automating Alignment ResearchΩ,Ian McKenzie,1mo
401,Interspecies diplomacy as a potentially productive lens on AGI alignmentΩ,Shariq Hashme,1mo
402,The Case Against Digital Sentience: A Summary,lucas11,1mo
403,Why are some problems Super Hard?Q,Gabriel Alfour,1mo
404,Adversarial epistemology,jchan,1mo
405,"Ethan Perez on the Inverse Scaling Prize, Language Feedback and Red TeamingΩ",Michaël Trazzi,1mo
406,Schelling Points in Thing-Space,Richard Henage,1mo
407,Thoughts about OOD alignment,Dmitry Savishchev,1mo
408,It Looks Like You’re Trying To Take Over The Narrative,George3d6,1mo
409,History of Zero-based Months?,jefftk,1mo
410,Lies on the internet,sudo -i,1mo
411,AGI Timelines Are Mostly Not Strategically Relevant To AlignmentΩ,johnswentworth,1mo
412,Why does AGI need a utility function?Q,randomstring,1mo
413,"Progress links and tweets, 2022-08-23",jasoncrawford,1mo
414,AI alignment as “navigating the space of intelligent behaviour”Ω,Nora_Ammann,1mo
415,everything is okay,carado,1mo
416,Nate Soares' Life Advice,CatGoddess,1mo
417,Discussion on utilizing AI for alignment,elifland,1mo
418,Digital Dinner Signup,jefftk,1mo
419,Finding Goals in the World ModelΩ,"Jeremy Gillen, JamesH, Thomas Larsen",1mo
420,Please (re)explain your personal jargon,Nathan Helm-Burger,1mo
421,On Car Seats as Contraception,Zvi,1mo
422,"AI art isn't ""about to shake things up"". It's already here.",Davis_Kingsley,1mo
423,What if we solve AI Safety but no one caresQ,142857,1mo
424,AXRP Episode 17 - Training for Very High Reliability with Daniel ZieglerΩ,DanielFilan,1mo
425,Stable Diffusion has been released,P.,1mo
426,Is there a Lesswrong newsletter I can subscribe to?Q,iamef,1mo
427,"Ways to increase working memory, and/or cope with low working memory?Q",NicholasKross,1mo
428,The Alignment Problem Needs More Positive Fiction,Netcentrica,1mo
429,The Bunny: An EA Short Story,JohnGreer,1mo
430,Beyond Hyperanthropomorphism,PointlessOne,1mo
431,"If you know you are unlikely to change your mind, should you be lazier when researching?Q",Double,1mo
432,Pivotal acts using an unaligned AGI?,Simon Fischer,1mo
433,My Plan to Build Aligned Superintelligence,apollonianblues,1mo
434,Embracing the Opposition's Point,Yulia,1mo
435,How evolution succeeds and fails at value alignment,Ocracoke,1mo
436,"A First Attempt to Dissolve ""Is Consciousness Reducible?""",DragonGod,1mo
437,Volunteer to host a meetup!,mingyuan,1mo
438,What are the Limits on Computability?Q,DragonGod,1mo
439,Review: Amusing Ourselves to Death,LRudL,1mo
440,"Paper is published! 100,000 lumens to treat seasonal affective disorder",Fabienne,1mo
441,What's the Least Impressive Thing GPT-4 Won't be Able to Do,Algon,1mo
442,Are you allocated optimally in your own estimation?Q,Emrik,1mo
443,Broad Picture of Human ValuesΩ,Thane Ruthenis,1mo
444,"So, I Want to Be a ""Thinkfluencer""",DragonGod,1mo
445,The 'Bitter Lesson' is Wrong,deepthoughtlife,1mo
446,Refine's Second Blog Post DayΩ,adamShimi,1mo
447,No One-Size-Fit-All Epistemic StrategyΩ,adamShimi,1mo
448,What if we approach AI safety like a technical engineering safety problemΩ,zeshen,1mo
449,"Vague concepts, family resemblance and cluster properties ",Q Home,1mo
450,PreDCA: vanessa kosoy's alignment protocolΩ,carado,1mo
451,Benchmarking Proposals on Risk ScenariosΩ,Paul Bricman,1mo
452,[About Me] Cinera's Home Page,DragonGod,1mo
453,The Loire Is Not Dry,jefftk,1mo
454,"How to do theoretical research, a personal perspectiveΩ",Mark Xu,1mo
455,Can You Upload Your Mind & Live Forever? From Kurzgesagt - In a Nutshell,Noosphere89,1mo
456,"Guided Consumption Theory: A Virtuous Dance between Altruistic Agents, Economic Discriminators, and Opportunistic Helpers",Brad West ,1mo
457,Epistemic Artefacts of (conceptual) AI alignment researchΩ,Nora_Ammann,1mo
458,AI Safety bounty for practical homomorphic encryption,acylhalide,1mo
459,"Statistics for vague concepts and ""Colors"" of places",Q Home,1mo
460,Are language models close to the superhuman level in philosophy?Q,Roman Leventov,1mo
461,What does moral progress consist of?,jasoncrawford,1mo
462,Introducing the Existential Risks Introductory Course (ERIC),"TheMcDouglas, Nandini Shiralkar",1mo
463,Alignment's phlogiston ,Eleni Angelou,1mo
464,A conversation about progress and safety,jasoncrawford,1mo
465,Discovering AgentsΩ,zac_kenton,1mo
466,Oops It's Time To Overthrow the Organizer Day!,Screwtape,1mo
467,Bias towards simple functions; application to alignment? ,DavidHolmes,1mo
468,In Defense Of Making Money,George3d6,1mo
469,Playing Without Affordances,Alex Hollow,1mo
470,Goal-directedness: relativising complexity,Morgan_Rogers,1mo
471,Matt Yglesias on AI Policy,Grant Demaree,1mo
472,Announcing the Distillation for Alignment Practicum (DAP),"Jonas Hallgren, TheMcDouglas",1mo
473,What Games These Days?,jefftk,1mo
474,Covid 8/18/22: CDC Admits Mistakes,Zvi,1mo
475,What's up with the bad Meta projects?,Yitz,1mo
476,"Announcing Encultured AI: 
Building a Video GameΩ","Andrew_Critch, Nick Hay",1mo
477,Spoons and Myofascial Trigger Points,vitaliya,1mo
478,Concrete Advice for Forming Inside Views on AI SafetyΩ,Neel Nanda,1mo
479,"Progress links and tweets, 2022-08-17",jasoncrawford,1mo
480,"Conditioning, Prompts, and Fine-TuningΩ",Adam Jermyn,1mo
481,The Core of the Alignment Problem is...Ω,"Thomas Larsen, Jeremy Gillen, JamesH",1mo
482,Could the simulation argument also apply to dreams?Q,Nathan1123,1mo
483,Interpretability Tools Are an Attack Channel,Thane Ruthenis,1mo
484,Human Mimicry Mainly Works When We’re Already CloseΩ,johnswentworth,1mo
485,Thoughts on 'List of Lethalities',Alex Lawsen ,1mo
486,The longest training runΩ,"Jsevillamol, Tamay, Owen Dudney, anson.ho",1mo
487,Autonomy as taking responsibility for reference maintenanceΩ,Ramana Kumar,1mo
488,Insufficient awareness of how everything sucks,Flaglandbase,1mo
489,Mesa-optimization for goals defined only within a training environment is dangerous,Rubi,1mo
490,Spoiler-Free Review: Across the Obelisk,Zvi,1mo
491,Duplicating Rasberry Pi Images,jefftk,1mo
492,My thoughts on direct work (and joining LessWrong),RobertM,1mo
493,We can make the future a million years from now go better [video],Writer,1mo
494,The Open Society and Its Enemies: Summary and Thoughts,matto,1mo
495,An introduction to signalling theory,Mvolz,1mo
496,Understanding differences between humans and intelligence-in-general to build safe AGI,Florian_Dietz,1mo
497,Against population ethics,jasoncrawford,1mo
498,Deception as the optimal: mesa-optimizers and inner alignment  ,Eleni Angelou,1mo
499,Crowdsourcing Anki Decks,Arden P. B. Wiese,1mo
500,What Makes an Idea Understandable? On Architecturally and Culturally Natural Ideas. ,"NickyP, Peter S. Park, Stephen Fowler",1mo
501,Dwarves & D.Sci: Data Fortress Evaluation & Ruleset,aphyer,1mo
502,I’m mildly skeptical that blindness prevents schizophrenia,Steven Byrnes,1mo
503,"What's General-Purpose Search, And Why Might We Expect To See It In Trained ML Systems?Ω",johnswentworth,1mo
504,"""What Mistakes Are You Making Right Now?""",David Udell,1mo
505,Limits of Asking ELK if Models are Deceptive,Oam Patel,1mo
506,On Preference Manipulation in Reward Learning Processes,Felix Hofstätter,1mo
507,Capital and inequality ,NathanBarnard,1mo
508,Are there practical exercises for developing the Scout mindset?Q,ChristianKl,1mo
509,The Parable of the Boy Who Cried 5% Chance of Wolf,KatWoods,1mo
510,Extreme Security,lc,1mo
511,No shortcuts to knowledge: Why AI needs to ease up on scaling and learn how to code,Yldedly,1mo
512,A Mechanistic Interpretability Analysis of GrokkingΩ,"Neel Nanda, Tom Lieberum",5d
513,If a nuke is coming towards SF Bay can people bunker in BART tunnels?Q,"DonyChristie, rhollerith_dot_com",1mo
514,How do you get a job as a software developer?Q,"lsusr, Dagon",1mo
515,And the Revenues Are So Small,Zvi,1mo
516,Seeking Interns/RAs for Mechanistic Interpretability ProjectsΩ,Neel Nanda,1mo
517,"What is the probability that a superintelligent, sentient AGI is actually infeasible?Q",Nathan1123,1mo
518,Dealing With Delusions,adrusi,1mo
519,All the posts I will never writeΩ,Self-Embedded Agent,1mo
520,AI Transparency: Why it’s critical and how to obtain it.,Zohar Jackson,1mo
521,A brief note on Simplicity BiasΩ,Spencer Becker-Kahn,1mo
522,"Brain-like AGI project ""aintelope"" Ω",Gunnar_Zarncke,1mo
523,Evolution is a bad analogy for AGI: inner alignmentΩ,Quintin Pope,1mo
524,An Uncanny Prison,Nathan1123,1mo
525,Cultivating Valiance,Shos Tekofsky,1mo
526,An extended rocket alignment analogyΩ,remember,1mo
527,What is an agent in reductionist materialism?Q,Valentine,1mo
528,Refine's First Blog Post DayΩ,adamShimi,1mo
529,The Dumbest Possible Gets There FirstΩ,Artaxerxes,1mo
530,I missed the crux of the alignment problem the whole timeΩ,zeshen,1mo
531,goal-program bricksΩ,carado,1mo
532,Shapes of Mind and Pluralism in AlignmentΩ,adamShimi,1mo
533,How I think about alignmentΩ,Linda Linsefors,1mo
534,Steelmining via AnalogyΩ,Paul Bricman,1mo
535,the Insulated Goal-Program ideaΩ,carado,1mo
536,Appendix: Jargon Dictionary,CFAR!Duncan,1mo
537,Florida Elections,Double,1mo
538,The OpenAI playground for GPT-3 is a terrible interface. Is there any great local (or web) app for exploring/learning with language models?Q,aviv,1mo
539,Infant AI Scenario,Nathan1123,1mo
540,DeepMind alignment team opinions on AGI ruin argumentsΩ,Vika,8d
541,Dissolve: The Petty Crimes of Blaise Pascal,JohnBuridan,1mo
542,What is estimational programming? Squiggle in context,Quinn,1mo
543,Oversight Misses 100% of Thoughts The AI Does Not ThinkΩ,johnswentworth,1mo
544,Timelines explanation post part 1 of ?,Nathan Helm-Burger,1mo
545,A little playing around with Blenderbot3,Nathan Helm-Burger,1mo
546,Refining the sharp left turn threat modelΩ,"Vika, Vikrant Varma, Ramana Kumar, Mary Phuong",1mo
547,Argument by Intellectual Ordeal,lc,1mo
548,Anti-squatted AI x-risk domains index,plex,1mo
549,What kind of moral framework would you spread if you could decide? ,NinaR,1mo
550,What are some good arguments against building new nuclear power plants?Q,RomanS,1mo
551,Seeking PCK (Pedagogical Content Knowledge),CFAR!Duncan,1mo
552,Artificial intelligence wireheading,Big Tony,1mo
553,The Host Minds of HBO's Westworld.,Nerret,1mo
554,Perfect PredictorsQ,aditya malik,1mo
555,"Seriously, what goes wrong with ""reward the agent when it makes you smile""?QΩ","TurnTrout, johnswentworth",1mo
556,"Encultured AI Pre-planning, Part 2: 
Providing a ServiceΩ","Andrew_Critch, Nick Hay",1mo
557,My summary of the alignment problem,Peter Hroššo,1mo
558,Language models seem to be much better than humans at next-token predictionΩ,"Buck, Fabien, LawrenceC",1mo
559,Introducing Pastcasting: A tool for forecasting practice,Sage Future,1mo
560,"Pendulums, Policy-Level Decisionmaking, Saving State",CFAR!Duncan,1mo
561,Thoughts on the good regulator theorem,JonasMoss,1mo
562,How and why to turn everything into audio,"KatWoods, AmberDawn",1mo
563,Shard Theory: An OverviewΩ,David Udell,1mo
564,Do advancements in Decision Theory point towards moral absolutism?Q,Nathan1123,1mo
565,Covid 8/11/22: The End Is Never The End,Zvi,1mo
566,The alignment problem from a deep learning perspectiveΩ,Richard_Ngo,1mo
567,How much alignment data will we need in the long run?Ω,Jacob_Hilton,1mo
568,Formalizing Alignment,Marv K,1mo
569,"How Do We Align an AGI Without Getting Socially Engineered? 
(Hint: Box It)Ω","Peter S. Park, NickyP, Stephen Fowler",1mo
570,Emergent Abilities of Large Language Models [Linkpost],aogara,1mo
571,How To Go From Interpretability To Alignment: Just Retarget The SearchΩ,johnswentworth,1mo
572,Using GPT-3 to augment human intelligence,Henrik Karlsson,1mo
573,Dissent Collusion,Screwtape,2mo
574,"On Ego,  Reincarnation, Consciousness and The Universe ",qmaury,1mo
575,The Medium Is The Bandage,party girl,2mo
576,Why is increasing public awareness of AI safety not a priority?Q,FinalFormal2,2mo
577,Proposal: Consider not using distance-direction-dimension words in abstract discussions,moridinamael,2mo
578,"How would two superintelligent AIs interact, if they are unaligned with each other?Q",Nathan1123,2mo
579,"Disagreements about Alignment: Why, and how, we should try to solve them",ojorgensen,2mo
580,"Progress links and tweets, 2022-08-09",jasoncrawford,2mo
581,Content generation. Where do we draw the line?,Q Home,2mo
582,What are some alternatives to Shapley values which drop additivity?Q,eapi,2mo
583,Team Shard Status Report,David Udell,2mo
584,Announcing: Mechanism Design for AI Safety - Reading GroupΩ,Rubi,2mo
585,"What are some Works that might be useful but are difficult, so forgotten?
Q",TekhneMakre,2mo
586,"Project proposal: Testing the IBP definition of agent
","Jeremy Gillen, Thomas Larsen, JamesH",2mo
587,How (not) to choose a research project,"Garrett Baker, CatGoddess, Johannes C. Mayer",2mo
588,"Are ya winning, son?Q",Nathan1123,2mo
589,Manifold x CSPI $25k Forecasting Tournament,David Chee,2mo
590,Is it possible to find venture capital for AI research org with strong safety focus?Q,AnonResearch,2mo
591,Many Gods refutation and Instrumental Goals. (Proper one)Q,aditya malik,2mo
592,Radio Bostrom: Audio narrations of papers by Nick Bostrom,PeterH,2mo
593,"Encultured AI, Part 1 Appendix: Relevant Research ExamplesΩ","Andrew_Critch, Nick Hay",2mo
594,"Encultured AI Pre-planning, Part 1: 
Enabling New BenchmarksΩ","Andrew_Critch, Nick Hay",2mo
595,Broad Basins and Data Compression,"Jeremy Gillen, Stephen Fowler, Thomas Larsen",2mo
596,Interpretability/Tool-ness/Alignment/Corrigibility are not ComposableΩ,johnswentworth,2mo
597,A sufficiently paranoid paperclip maximizer,RomanS,2mo
598,Instrumental Goals and Many Gods Refutation Q,aditya malik,2mo
599,"Area under the curve, Eat Dirt, Broccoli Errors, Copernicus & Chaos",CFAR!Duncan,2mo
600,Steganography in Chain of Thought ReasoningΩ,A Ray,2mo
601,How Deadly Will Roughly-Human-Level AGI Be?,David Udell,2mo
602,Experiment: Be my math tutor? ,sudo -i,2mo
603,"Complexity No Bar to AI (Or, why Computational Complexity doesn't matter for real life problems)",Noosphere89,2mo
604,The lessons of Xanadu,jasoncrawford,2mo
605,Careful with Caching,jefftk,2mo
606,How would Logical Decision Theories address the Psychopath Button?Q,Nathan1123,2mo
607,Jack Clark on the realities of AI policy,Kaj_Sotala,2mo
608,Expected (Social) Value,algrthms,2mo
609,"Lamentations, Gaza and Empathy",Yair Halberstadt,2mo
610,Paper reading as a Cargo Cult,jem-mosig,2mo
611,Most Ivy-smart students aren't at Ivy-tier schools,Aaron Bergman,2mo
612,Do meta-memes and meta-antimemes exist? e.g. 'The map is not the territory' is also a map,M. Y. Zuo,2mo
613,Can we get full audio for Eliezer's conversation with Sam Harris?Q,jskatt,2mo
614,Newcombness of the Dining Philosophers Problem,Nathan1123,2mo
615,Dwarves & D.Sci: Data Fortress,aphyer,2mo
616,A Deceptively Simple Argument in favor of Problem Factorization,Logan Zoellner,2mo
617,A Data limited futureΩ,Donald Hobson,2mo
618,Six weeks doesn’t make a habit ,lynettebye,2mo
619,Why I Am Skeptical of AI Regulation as an X-Risk Mitigation Strategy,A Ray,2mo
620,My advice on finding your own path,A Ray,2mo
621,Metaculus and medians,rossry,2mo
622,Announcing the Introduction to ML Safety courseΩ,"Dan Hendrycks, ThomasW, Oliver Zhang",2mo
623,"«Boundaries», Part 2: trends in EA's handling of boundaries",Andrew_Critch,2mo
624,[AMA] Announcing Open Phil’s University Group Organizer and Century Fellowships [x-post],"abergal, ClaireZabel",2mo
625,Boston Rents Over Time II,jefftk,2mo
626,PredictIt is closing due to CFTC changing its mind,eigen,2mo
627,"""Just hiring people"" is sometimes still actually possible",lc,2mo
628,The need for certainty,Thomas McMurtry,2mo
629,Rant on Problem Factorization for AlignmentΩ,johnswentworth,2mo
630,Counterfactuals are Confusing because of an Ontological ShiftΩ,Chris_Leong,2mo
631,"Gears-Level Understanding, Deliberate Performance, The Strategic Level",CFAR!Duncan,2mo
632,Post-mortem?Q,gwern,2mo
633,Where are the red lines for AI?,Karl von Wendt,2mo
634,Bridging Expected Utility Maximization and OptimizationΩ,Whispermute,2mo
635,Deontology and Tool AI,Nathan1123,2mo
636,An attempt to understand the Complexity of Values,Dalton Mabery,2mo
637,"What drives progress, theory or application?Q",berglund,2mo
638,The Falling Drill,Screwtape,2mo
639,Convergence Towards World-Models: A Gears-Level ModelΩ,Thane Ruthenis,2mo
640,$20K In Bounties for AI Safety Public MaterialsΩ,"Dan Hendrycks, ThomasW, Oliver Zhang",2mo
641,Two Kids Crosswise,jefftk,2mo
642,Cambist Booking,Screwtape,2mo
643,Calibration Trivia,Screwtape,2mo
644,Monthly Shorts 7/22,Celer,2mo
645,The Pragmascope IdeaΩ,johnswentworth,2mo
646,Running a Basic Meetup,Screwtape,2mo
647,"Fiber arts, mysterious dodecahedrons, and waiting on “Eureka!”",eukaryote,2mo
648,"Would ""Manhattan Project"" style be beneficial or deleterious for AI Alignment?Q",Just Learning,2mo
649,AI alignment: Would a lazy self-preservation instinct be sufficient?Q,BrainFrog,2mo
650,"Socratic Ducking, OODA Loops, Frame-by-Frame Debugging",CFAR!Duncan,2mo
651,What do ML researchers think about AI in 2022?,KatjaGrace,2mo
652,Interpretability isn’t Free,Joel Burget,2mo
653,"High Reliability Orgs, and AI Companies",Raemon,2mo
654,"Surprised by ELK report's counterexample to Debate, IDA",Evan R. Murphy,2mo
655,Covid 8/4/22: Rebound,Zvi,2mo
656,Clapping Lower,jefftk,2mo
657,"How do I know if my first post should be a post, or a question?Q",Nathan1123,2mo
658,Precursor checking for deceptive alignmentΩ,evhub,2mo
659,Transformer language models are doing something more general,Numendil,2mo
660,Some doubts about Non Superintelligent AIsQ,aditya malik,2mo
661,Announcing Squiggle: Early Access,ozziegooen,2mo
662,Survey: What (de)motivates you about AI risk?,Daniel_Friedrich,2mo
663,Externalized reasoning oversight: a research direction for language model alignmentΩ,tamera,2mo
664,How does one recognize information and differentiate it from noise? Q,M. Y. Zuo,2mo
665,Law-Following AI 4: Don't Rely on Vicarious LiabilityΩ,Cullen_OKeefe,2mo
666,Two-year update on my personal AI timelinesΩ,Ajeya Cotra,2mo
667,My takeaways from the EA In-depth 4th week discussion: Animal Welfare,Kriz Royce Tahimic,2mo
668,Open & Welcome Thread - Aug/Sep 2022,Thomas,2mo
669,What are the Red Flags for Neural Network Suffering? - Seeds of Science call for reviewers,rogersbacon,2mo
670,Againstness,CFAR!Duncan,2mo
671,(Summary) Sequence Highlights - Thinking Better on Purpose,qazzquimby,2mo
672,"Progress links and tweets, 2022-08-02",jasoncrawford,2mo
673,Thinking without priors?,Q Home,2mo
674,Turbocharging,CFAR!Duncan,2mo
675,"I want to donate some money (not much, just what I can afford) to AGI Alignment research, to whatever organization has the best chance of making sure that AGI goes well and doesn't kill us all. What are my best options, where can I make the most difference per dollar?Q","lumenwrites, Richard_Ngo",2mo
676,Would quantum immortality mean subjective immortality?Q,n0ah,2mo
677,Letter from leading Soviet Academicians to party and government leaders of the Soviet Union regarding signs of decline and structural problems of the economic-political system (1970),M. Y. Zuo,2mo
678,Is there any writing about prompt engineering for humans?Q,Alex Hollow,2mo
679,Meditation course claims 65% enlightenment rate: my review,KatWoods,2mo
680,Which intro-to-AI-risk text would you recommend to...Q,Sherrinford,2mo
681,"Polaris, Five-Second Versions, and Thought Lengths",CFAR!Duncan,2mo
682,"A Word is Worth 1,000 Pictures",Kully,2mo
683,On akrasia: starting at the bottom,seecrow,2mo
684,How likely do you think worse-than-extinction type fates to be?Q,span1,2mo
685,Don't be a Maxi,Cole Killian,2mo
686,Technical AI Alignment Study Group,Eric K,2mo
687,Abstraction sacrifices causal clarity,Marv K,2mo
688,Time-logging programs and/or spreadsheets (2022),mikbp,2mo
689,Conservatism is a rational response to epistemic uncertainty,contrarianbrit,2mo
690,Perverse Independence Incentives,jefftk,2mo
691,Wanted: Notation for credal resilience,PeterH,2mo
692,Anatomy of a Dating Document,squidious,2mo
693,chinchilla's wild implicationsΩ,nostalgebraist,2mo
694,Wolfram Research v Cook,Kenny,2mo
695,AGI-level reasoner will appear sooner than an agent; what the humanity will do with this reasoner is critical,Roman Leventov,2mo
696,Writing this post as rationality case study,Ben Amitay,2mo
697,How transparency changed over timeΩ,ViktoriaMalyasova,2mo
698,Drexler’s Nanotech Forecast,PeterMcCluskey,2mo
699,What job should I do?Q,Tom Paine,2mo
700,Translating between Latent Spaces,"JamesH, Jeremy Gillen, NickyP",2mo
701,Humans Reflecting on HRHΩ,leogao,2mo
702,Comparing Four Approaches to Inner AlignmentΩ,Lucas Teixeira,2mo
703,Questions for a Theory of Narratives,Marv K,2mo
704,Focusing,CFAR!Duncan,2mo
705,Abstracting The Hardness of Alignment: Unbounded Atomic OptimizationΩ,adamShimi,2mo
706,Bucket Errors,CFAR!Duncan,2mo
707,Distillation Contest - Results and Recap,Aris,2mo
708,The generalized Sierpinski-Mazurkiewicz theorem.,Donald Hobson,2mo
709,Conjecture: Internal Infohazard PolicyΩ,"Connor Leahy, Sid Black, Chris Scammell, Andrea_Miotti",2mo
710,The Conversations We Make Space For,Severin T. Seehrich,2mo
711,Defining Optimization in a Deeper Way Part 4,Jemist,2mo
712,"For Better Commenting, Stop Out Loud",AllAmericanBreakfast,2mo
713,"Announcing the AI Safety Field Building Hub, a new effort to provide AISFB projects, mentorship, and funding",Vael Gates,2mo
714,Covid 7/28/22: Ruining It For Everyone,Zvi,2mo
715,Monkeypox Post #2,Zvi,2mo
716,Seeking beta readers who are ignorant of biology but knowledgeable about AI safety,Holly_Elmore,2mo
717,Principles of Privacy for Alignment ResearchΩ,johnswentworth,2mo
718,Moral strategies at different capability levelsΩ,Richard_Ngo,2mo
719,"Progress links and tweets, 2022-07-27",jasoncrawford,2mo
720,Quantum Advantage in Learning from Experiments,Dennis Towne,2mo
721,Levels of PluralismΩ,adamShimi,2mo
722,Human trials for the Marburg vaccine: funding opportunity?,americanwalrus,2mo
723,“Fanatical” Longtermists: Why is Pascal’s Wager wrong?Q,Yitz,2mo
724,Unifying Bargaining Notions (2/2)Ω,Diffractor,2mo
725,AGI ruin scenarios are likely (and disjunctive)Ω,So8res,2mo
726,Technocracy and the Space Age,jasoncrawford,2mo
727,"«Boundaries», Part 1: a key missing concept from utility theoryΩ",Andrew_Critch,2mo
728,Incoherence of unbounded selfishness,emmab,2mo
729,«Boundaries» Sequence (Index Post),Andrew_Critch,2mo
730,Active Inference as a formalisation of instrumental convergenceΩ,Roman Leventov,2mo
731,NeurIPS ML Safety Workshop 2022Ω,Dan Hendrycks,2mo
732,Utility functions and probabilities are entangled,Thomas Kwa,2mo
733,Alignment being impossible might be better than it being really difficult,Martín Soto,2mo
734,AI ethics vs AI alignment,Wei_Dai,2mo
735,How Promising is Theoretical Research on Rationality? Seeking Career Advice,Aspirant223,2mo
736,How optimistic should we be about AI figuring out how to interpret itself?Q,oh54321,2mo
737,Protectionism in One Country: How Industrial Policy Worked in Canada,Davis Kedrosky,2mo
738,Mistakes as agency,pchvykov,2mo
739,My Bitcoin Thesis @2022 - Part 1,aysajan,2mo
740,The Reader's Guide to Optimal Monetary Policy,Ege Erdil,2mo
741,AGI Safety Needs People With All Skillsets!,Severin T. Seehrich,2mo
742,Opening Session Tips & Advice,CFAR!Duncan,2mo
743,How much should we worry about mesa-optimization challenges?,sudo -i,2mo
744,Does agent foundations cover all future ML systems?Q,Jonas Hallgren,2mo
745,Unifying Bargaining Notions (1/2)Ω,Diffractor,16d
746,Reward is not the optimization targetΩ,TurnTrout,2mo
747,Brainstorm of things that could force an AI team to burn their leadΩ,So8res,2mo
748,Is there any evidence that handwashing does anything to prevent COVID?Q,mukashi,2mo
749,Finding Skeletons on Rashomon Ridge,"David Udell, Peter S. Park, NickyP",2mo
750,Gathering Information you won't use directly is often useful,Johannes C. Mayer,2mo
751,"Impact of "" 'Let's think step by step' is all you need""?Q",yrimon,2mo
752,The Most Important Century: The Animation,"Writer, Matthew Barnett",2mo
753,Hiring Programmers in Academia,jefftk,2mo
754,Relationship between subjective experience and intelligence?,Q Home,2mo
755,Double Crux,CFAR!Duncan,2mo
756,Example Meetup Description,Julius,2mo
757,Eavesdropping on Aliens: A Data Decoding Challenge,anonymousaisafety,2mo
758,"Information theoretic model analysis may not lend much insight, but we may have been doing them wrong!",Garrett Baker,2mo
759,What's next for instrumental rationality?,Andrew_Critch,2mo
760,Easy guide for running a local Rationality meetup,Nikita Sokolsky,2mo
761,"Curating ""The Epistemic Sequences"" (list v.0.1)",Andrew_Critch,2mo
762,A Bias Against Altruism,Conor Sullivan,2mo
763,What Environment Properties Select Agents For World-Modeling?,Thane Ruthenis,2mo
764,Which singularity schools plus the no singularity school was right?,Noosphere89,2mo
765,Basic Post Scarcity Q&A,Lorenzo Rex,2mo
766,Robustness to Scaling Down: More Important Than I ThoughtΩ,adamShimi,2mo
767,Eating Boogers,George3d6,2mo
768,"On Akrasia, Habits and Reward Maximization",Aiyen,2mo
769,Which values are stable under ontology shifts?,Richard_Ngo,2mo
770,Trying out Prompt Engineering on TruthfulQA,Megan Kinniment,2mo
771,"Symbolic distillation, Diffusion, Entropy, Replicators, Agents, oh my (a mid-low quality thinking out loud post)",the gears to ascenscion,2mo
772,Room Opening,jefftk,2mo
773,"Connor Leahy on Dying with Dignity, EleutherAI and Conjecture",Michaël Trazzi,2mo
774,Wyclif's Dust: the missing chapter,David Hugh-Jones,2mo
775,Making DALL-E Count,AllAmericanBreakfast,2mo
776,Internal Double Crux,CFAR!Duncan,2mo
777,One-day applied rationality workshop in Berlin Aug 29 (after LWCW),Duncan_Sabien,2mo
778,Conditioning Generative Models with RestrictionsΩ,Adam Jermyn,2mo
779,Our Existing Solutions to AGI Alignment (semi-safe),Michael Soareverix,2mo
780,Changing the world through slack & hobbies,Steven Byrnes,1mo
781,Which personalities do we find intolerable?,weathersystems,2mo
782,YouTubeTV and Spoilers,Zvi,2mo
783,How much to optimize for the short-timelines scenario?Q,SoerenMind,2mo
784,Is Gas Green?,ChristianKl,2mo
785,Why are politicians polarized?,ErnestScribbler,2mo
786,[AN #173] Recent language model results from DeepMindΩ,Rohin Shah,2mo
787,Don't take the organizational chart literally,lc,2mo
788,Personal forecasting retrospective: 2020-2022,elifland,2mo
789,Covid 7/21/22: Featuring ASPR,Zvi,2mo
790,Defining Optimization in a Deeper Way Part 3,Jemist,2mo
791,Cognitive Risks of Adolescent Binge Drinking,"Elizabeth, Martin Bernstorff",2mo
792,Why AGI Timeline Research/Discourse Might Be Overrated,Noosphere89,2mo
793,Enlightenment Values in a Vulnerable World,Maxwell Tabarrok,2mo
794,Countering arguments against working on AI safety,Rauno Arike,2mo
795,A Short Intro to Humans,Ben Amitay,2mo
796,How to Diversify Conceptual Alignment: the Model Behind RefineΩ,adamShimi,2mo
797,AI Safety Cheatsheet / Quick Reference,Zohar Jackson,2mo
798,What are the simplest questions in applied rationality where you don't know the answer to? Q,ChristianKl,2mo
799,Getting Unstuck on Counterfactuals,Chris_Leong,2mo
800,Pitfalls with Proofs,scasper,2mo
801,A daily routine I do for my AI safety research work,scasper,2mo
802,"Progress links and tweets, 2022-07-19",jasoncrawford,2mo
803,Abram Demski's ELK thoughts and proposal - distillationΩ,Rubi,2mo
804,Bounded complexity of solving ELK and its implicationsΩ,Rubi,2mo
805,A Critique of AI Alignment Pessimism,ExCeph,2mo
806,Ars D&D.Sci: Mysteries of Mana Evaluation & Ruleset,aphyer,2mo
807,Marburg Virus Pandemic Prediction Checklist,AllAmericanBreakfast,2mo
808,Applications are open for CFAR workshops in Prague this fall!,JohnSteidley,2mo
809,Sexual Abuse attitudes might be infohazardous,Pseudonymous Otter,2mo
810,Spending Update 2022,jefftk,2mo
811,Help ARC evaluate capabilities of current language models (still need people)Ω,Beth Barnes,2mo
812,At what point will we know if Eliezer’s predictions are right or wrong? ,anonymous123456,2mo
813,Modelling Deception,Garrett Baker,2mo
814,Are Intelligence and Generality Orthogonal?,cubefox,2mo
815,"Without specific countermeasures, the easiest path to transformative AI likely leads to AI takeoverΩ",Ajeya Cotra,2mo
816,Turning Some Inconsistent Preferences into Consistent Ones,niplav,2mo
817,Machine Learning Model Sizes and the Parameter Gap [abridged],Pablo Villalobos,2mo
818,Quantilizers and Generative ModelsΩ,Adam Jermyn,2mo
819,AI Hiroshima (Does A Vivid Example Of Destruction Forestall Apocalypse?),Sable,2mo
820,How the ---- did Feynman Get Here !?,George3d6,2mo
821,Conditioning Generative Models for AlignmentΩ,Jozdien,2mo
822,Training goals for large language modelsΩ,Johannes_Treutlein,2mo
823,Reward models can act like they are deceptively aligned,Josh,2mo
824,A distillation of Evan Hubinger's training stories (for SERI MATS)Ω,Daphne_W,2mo
825,Forecasting ML Benchmarks in 2023Ω,jsteinhardt,2mo
826,Addendum: A non-magical explanation of Jeffrey Epstein,lc,2mo
827,"Launching a new progress institute, seeking a CEO",jasoncrawford,2mo
828,Why you might expect homogeneous take-off: evidence from ML researchΩ,Andrei Alexandru,2mo
829,Examples of AI Increasing AI ProgressΩ,ThomasW,2mo
830,Four questions I ask AI safety researchers,Akash,2mo
831,Why I Think Abrupt AI Takeoff,lincolnquirk,2mo
832,Resolve Cycles,CFAR!Duncan,2mo
833,Culture wars in riddle format,Malmesbury,2mo
834,Alignment as Game Design,Shos Tekofsky,2mo
835,Risk Management from a Climbers Perspective,Annapurna,2mo
836,"Cognitive Instability, Physicalism, and Free Will",dadadarren,2mo
837,All AGI safety questions welcome (especially basic ones) [July 2022],"plex, Robert Miles",2mo
838,QNR Prospects,PeterMcCluskey,2mo
839,To-do waves,Paweł Sysiak,2mo
840,"A summary of every ""Highlights from the Sequences"" post",Akash,2mo
841,Moneypumping Bryan Caplan's Belief in Free Will,Morpheus,2mo
842,Safety Implications of LeCun's path to machine intelligenceΩ,Ivan Vendrov,2mo
843,Comfort Zone Exploration,CFAR!Duncan,2mo
844,A time-invariant version of Laplace's rule,"Jsevillamol, Ege Erdil",2mo
845,An attempt to break circularity in science,fryloysis,2mo
846,Highlights from the memoirs of Vannevar Bush,jasoncrawford,2mo
847,Notes on Learning the PriorΩ,Spencer Becker-Kahn,2mo
848,Review of The Engines of Cognition,William Gasarch,2mo
849,A review of Nate Hilger's The Parent Trap,David Hugh-Jones,2mo
850,Musings on the Human Objective Function,Michael Soareverix,2mo
851,Peter Singer's first published piece on AI ,Fai,2mo
852,Don't use 'infohazard' for collectively destructive info,Eliezer Yudkowsky,2mo
853,Upcoming heatwave: advice,stavros,2mo
854,A note about differential technological developmentΩ,So8res,2mo
855,Inward and outward steelmanning,Q Home,2mo
856,Potato diet: A post mortem and an answer to SMTM's article,joy_void_joy,2mo
857,A story about a duplicitous API,LiLiLi,2mo
858,Proposed Orthogonality Theses #2-5,rjbg,2mo
859,Circumventing interpretability: How to defeat mind-readersΩ,Lee Sharkey,2mo
860,Criticism of EA Criticism Contest,Zvi,2mo
861,Humans provide an untapped wealth of evidence about alignmentΩ,"TurnTrout, Quintin Pope",1mo
862,"Wacky, risky, anti-inductive intelligence-enhancement methods?Q","NicholasKross, Quintin Pope",2mo
863,How to impress students with recent advances in ML?Q,Raphaël S,2mo
864,Notes on Love,David Gross,2mo
865,Better Quiddler,jefftk,2mo
866,Covid 7/14/22: BA.2.75 Plus Tax,Zvi,2mo
867,Deep learning curriculum for large language model alignmentΩ,Jacob_Hilton,2mo
868,Artificial Sandwiching: When can we test scalable alignment protocols without humans?Ω,Sam Bowman,2mo
869,Any tips for eliciting one's own latent knowledge?Q,MSRayne,2mo
870,Goal Alignment Is Robust To the Sharp Left Turn,Thane Ruthenis,2mo
871,Making decisions using multiple worldviews,Richard_Ngo,2mo
872,App idea to help with reading STEM textbooks (feedback request)Q,AllAmericanBreakfast,2mo
873,MIRI Conversations: Technology Forecasting & Gradualism (Distillation),TheMcDouglas,2mo
874,How could the universe be infinitely large?Q,amarai,2mo
875,John von Neumann on how to safely progress with technology,Dalton Mabery,2mo
876,Everyone is an Imposter ,Tharin,2mo
877,Which AI Safety research agendas are the most promising?QΩ,Chris_Leong,2mo
878,Straw-Steelmanning,Chris van Merwijk,2mo
879,Alien Message Contest: Solution,DaemonicSigil,2mo
880,"Can what I am good at contribute on this forum? And if yes, how?Q",[anonymous],2mo
881,Passing Up Pay,jefftk,2mo
882,What is wrong with this approach to corrigibility?Q,Rafael Cosman,2mo
883,Acceptability Verification: A Research AgendaΩ,"David Udell, evhub",2mo
884,"Progress links and tweets, 2022-07-12",jasoncrawford,2mo
885,"Response to Blake Richards: AGI, generality, alignment, & loss functionsΩ",Steven Byrnes,2mo
886,Three Minimum Pivotal Acts Possible by Narrow AI,Michael Soareverix,2mo
887,Mosaic and Palimpsests: Two Shapes of ResearchΩ,adamShimi,2mo
888,How do you concisely communicate & navigate the politics / culture at your job working at a large corporation or institution?Q,Willa,2mo
889,On how various plans miss the hard bits of the alignment challengeΩ,So8res,2mo
890,Rainmaking,WalterL,2mo
891,Book Review: Neal Stephenson’s “Termination Shock”,Tyler Simmons,2mo
892,Defining Optimization in a Deeper Way Part 2,Jemist,2mo
893,"Marriage, the Giving What We Can Pledge, and the damage caused by vague public commitments",Jeffrey Ladish,2mo
894,Systemization,CFAR!Duncan,2mo
895,How Can I Maximize My Happiness?,Matt Goldwater,2mo
896,How do AI timelines affect how you live your life? Q,"Quadratic Reciprocity, Aditya",2mo
897,Checksum Sensor Alignment,lsusr,2mo
898,The Alignment Problem,lsusr,2mo
899,Announcing Future Forum - Apply Now,"wANIEL, freemany",2mo
900,Immanuel Kant and the Decision Theory App Store,Daniel Kokotajlo,2mo
901,"Avoid the abbreviation ""FLOPs"" – use ""FLOP"" or ""FLOP/s"" instead",Daniel_Eth,3mo
902,My Opportunity Costs,abstractapplic,3mo
903,Hessian and Basin volumeΩ,Vivek Hebbar,3mo
904,Taste & Shaping,CFAR!Duncan,3mo
905,"Comment on ""Propositions Concerning Digital Minds and Society""",Zack_M_Davis,3mo
906,We are now at the point of deepfake job interviews,Trevor1,3mo
907,"Metaculus is seeking experienced leaders, researchers & operators for high-impact roles",ChristianWilliams,2mo
908,Why Portland,Adam Zerner,3mo
909,Heaven: The last part of dystopia,Existism,3mo
910,Hope Can = Heaven ,Existism,3mo
911,Report from a civilizational observer on Earth,owencb,3mo
912,Grouped Loss may disfavor discontinuous capabilitiesΩ,Adam Jermyn,3mo
913,Train first VS prune first in neural networks.Ω,Donald Hobson,3mo
914,"Visualizing Neural networks, how to blame the biasΩ",Donald Hobson,3mo
915,Using Ngram to estimate depression prevalence over time,David Gross,3mo
916,"Making it harder for an AGI to ""trick"" us, with STVsΩ",Tor Økland Barstad,3mo
917,Ars D&D.sci: Mysteries of Mana,aphyer,3mo
918,Some thoughts on Animals,nitinkhanna,3mo
919,MATS Models,johnswentworth,3mo
920,I’ve become a medical mystery and I don’t know how to effectively get helpQ,"CraigMichael, AllAmericanBreakfast",3mo
921,Changes in Community Dynamics: A Follow-Up to 'The Berkeley Community & the Rest of Us',Evan_Gaensbauer,3mo
922,Research Notes: What are we aligning for?,Shos Tekofsky,3mo
923,User research as a barometer of software design,Adam Zerner,3mo
924,Reinforcement Learner Wireheading,Nate Showell,3mo
925,Exposition as science: some ideas for how to make progress,riceissa,3mo
926,In Search of Strategic Clarity,james.lucassen,3mo
927,Unbounded Intelligence Lottery,kman,3mo
928,What New Desktop Should I Buy?Q,Zvi,3mo
929,Being a donor for Fecal Microbiota Transplants (FMT): Do good & earn easy money (up to 180k/y),Anton Rodenhauser,3mo
930,Safety considerations for online generative modelingΩ,Sam Marks,3mo
931,Human values & biases are inaccessible to the genomeΩ,TurnTrout,3mo
932,Cooperation with and between AGI\'s,PeterMcCluskey,3mo
933,Aversion Factoring,CFAR!Duncan,3mo
934,Genders Discrimination,Jacob Falkovich,3mo
935,Consider Multiclassing,JustisMills,3mo
936,Babysitting as Parenting Trial?,jefftk,3mo
937,When Giving People Money Doesn’t Help,Zvi,3mo
938,Wealth as a source of technological stagnation?,alyssavance,3mo
939,Race Along Rashomon RidgeΩ,"Stephen Fowler, Peter S. Park, MichaelEinhorn",3mo
940,What one paper would you show to someone to get them excited about your field?Q,oh54321,3mo
941,Principles for Alignment/Agency ProjectsΩ,johnswentworth,3mo
942,Confusions in My Model of AI Risk,peterbarnett,3mo
943,​Some Adventures of a Curious Richard Feynman,Dalton Mabery,3mo
944,How to Become a World Historical Figure (Péladan's Dream),rogersbacon,3mo
945,Covid 7/7/22: Paxlovid at the Pharmacy,Zvi,3mo
946,Cognitive Dissonance on Cognitive Capability,niederman,3mo
947,Outer vs inner misalignment: three framingsΩ,Richard_Ngo,3mo
948,Four Societal Interventions to Improve our AGI Position,Rafael Cosman,3mo
949,Deep neural networks are not opaque.,jem-mosig,3mo
950,"How humanity would respond to slow takeoff, with takeaways from the entire COVID-19 pandemic ",Noosphere89,3mo
951,Should you write under a blog or your own name?Q,Dalton Mabery,3mo
952,Predicting Parental Emotional Changes?,jefftk,3mo
953,Forecasting Through Fiction,Yitz,3mo
954,"My vision of a good future, part I",Jeffrey Ladish,3mo
955,Understanding updatelessness in the context of EDT and CDT,Sylvester Kollin,3mo
956,Tarnished Guy who Puts a Num on it,Jacob Falkovich,3mo
957,Carrying the Torch: A Response to Anna Salamon by the Guild of the Rose,moridinamael,3mo
958,Introducing the Fund for Alignment Research (We're Hiring!)Ω,"AdamGleave, Scott Emmons, Ethan Perez, Claudia Shi",3mo
959,Imperial Russia was doing fine without the Soviets,Davis Kedrosky,3mo
960,A Pattern Language For Rationality,Vaniver,3mo
961,How to destroy the universe with a hypercomputer,Trevor Cappallo,3mo
962,The curious case of Pretty Good human inner/outer alignment,PavleMiha,3mo
963,When is it appropriate to use statistical models and probabilities for decision making ?,Younes Kamel,3mo
964,Goal Factoring,CFAR!Duncan,3mo
965,Assorted thoughts about abstraction ,Adam Zerner,3mo
966,[AN #172] Sorry for the long hiatus!Ω,Rohin Shah,3mo
967,Outline: The Rectifying of Maps,hamnox,3mo
968,"ITT-passing and civility are good; ""charity"" is bad; steelmanning is niche",Rob Bensinger,2mo
969,Seeking opinions on the current and forward state of cryptocurrencies.Q,jmh,3mo
970,Please help us communicate AI xrisk. It could save the world.,otto.barten,3mo
971,Benchmark for successful concept extrapolation/avoiding goal misgeneralizationΩ,Stuart_Armstrong,3mo
972,"Procedural Executive Function, Part 1",DaystarEld,3mo
973,Anthropic's SoLU (Softmax Linear Unit),Joel Burget,3mo
974,Book Review: The Righteous Mind,ErnestScribbler,3mo
975,My Most Likely Reason to Die Young is AI X-Risk,AISafetyIsNotLongtermist,3mo
976,"Is General Intelligence ""Compact""?",DragonGod,3mo
977,Remaking EfficientZero (as best I can)Ω,Hoagy,3mo
978,We Need a Consolidated List of Bad AI Alignment Solutions,Double,3mo
979,AI Forecasting: One Year In,jsteinhardt,3mo
980,A compressed take on recent disagreements,kman,3mo
981,New US Senate Bill on X-Risk Mitigation [Linkpost],Evan R. Murphy,3mo
982,Monthly Shorts 6/22,Celer,3mo
983,Decision theory and dynamic inconsistency,paulfchristiano,3mo
984,Five routes of access to scientific literature,AllAmericanBreakfast,3mo
985,Toni Kurz and the Insanity of Climbing Mountains,GeneSmith,1mo
986,Wonder and The Golden AI Rule,JeffreyK,3mo
987,Evolution Doesn’t Have Feelings,Matt Goldwater,3mo
988,Nature abhors an immutable replicator... usually,MSRayne,3mo
989,Post hoc justifications as Compression Algorithm,Johannes C. Mayer,3mo
990,SOMA - A story about Consciousness,Johannes C. Mayer,3mo
991,Can we achieve AGI Alignment by balancing multiple human objectives?,Ben Smith,3mo
992,Trigger-Action Planning,CFAR!Duncan,3mo
993,Sexual self-acceptance,Johannes C. Mayer,3mo
994,Which one of these two academic routes should I take to end up in AI Safety?Q,Martín Soto,3mo
995,Naive Hypotheses on AI Alignment,Shos Tekofsky,3mo
996,The Tree of Life: Stanford AI Alignment Theory of Change,Gabriel Mukobi,3mo
997,Welcome to Analogia! (Chapter 7),Justin Bullock,3mo
998,What about transhumans and beyond?Q,AlignmentMirror,3mo
999,Goal-directedness: tackling complexity,Morgan_Rogers,3mo
1000,Literature recommendations July 2022,ChristianKl,3mo
1001,Deontological Evil,lsusr,3mo
1002,Could an AI Alignment Sandbox be useful?,Michael Soareverix,3mo
1003,Five views of Bayes' Theorem,Adam Scherlis,3mo
1004,[Linkpost] Existential Risk Analysis in Empirical Research PapersΩ,Dan Hendrycks,3mo
1005,Agenty AGI – How Tempting?,PeterMcCluskey,3mo
1006,Follow along with Columbia EA's Advanced AI Safety Fellowship!,RohanS,3mo
1007,AXRP Episode 16 - Preparing for Debate AI with Geoffrey IrvingΩ,DanielFilan,3mo
1008,Examples of practical implications of Judea Pearl's Causality workQ,ChristianKl,3mo
1009,Minerva,Algon,3mo
1010,Disarming status,sano,3mo
1011,Paper: Forecasting world events with neural nets,"Owain_Evans, Dan Hendrycks, Joe Kwon",3mo
1012,Reframing the AI Risk,Thane Ruthenis,3mo
1013,"Limerence Messes Up Your Rationality Real Bad, Yo",Raemon,3mo
1014,[Link] On the paradox of tolerance in relation to fascism and online content moderation – Unstable Ontology,Kenny,3mo
1015,Trends in GPU price-performanceΩ,"Marius Hobbhahn, Tamay",3mo
1016,How to deal with non-schedulable one-off stimulus-response-pair-like situations when planning/organising projects?Q,mikbp,3mo
1017,What Is The True Name of Modularity?Ω,"TheMcDouglas, Lucius Bushnaq, Avery",3mo
1018,Defining Optimization in a Deeper Way Part 1,Jemist,3mo
1019,SafetywashingΩ,Adam Scholl,3mo
1020,AGI alignment with what?Q,AlignmentMirror,3mo
1021,Who is this MSRayne person anyway?,MSRayne,3mo
1022,Open & Welcome Thread - July 2022,Kaj_Sotala,3mo
1023,Forecasts are not enough,Ege Erdil,3mo
1024,Murphyjitsu: an Inner Simulator algorithm,CFAR!Duncan,3mo
1025,GPT-3 Catching Fish in Morse Code,Megan Kinniment,3mo
1026,Metacognition in the Rat,Jacob Falkovich,3mo
1027,On viewquakes,Dalton Mabery,3mo
1028,The Track Record of Futurists Seems ... Fine,HoldenKarnofsky,3mo
1029,Quick survey on AI alignment resources,frances_lorenz,3mo
1030,"[Linkpost] Solving Quantitative Reasoning Problems with
Language Models",Yitz,3mo
1031,Failing to fix a dangerous intersection,alyssavance,3mo
1032,Most Functions Have Undesirable Global Extrema,En Kepeig,3mo
1033,    Hedonistic Isotopes:,Trozxzr,3mo
1034,Abadarian Trades,David Udell,3mo
1035,Formal Philosophy and Alignment Possible ProjectsΩ,Whispermute,3mo
1036,Cultivating And Destroying Agency,hath,3mo
1037,Covid 6/30/22: Vaccine Update Update,Zvi,3mo
1038,How should I talk about optimal but not subgame-optimal play?Q,JamesFaville,3mo
1039,Gradient hacking: definitions and examplesΩ,Richard_Ngo,3mo
1040,"Progress links and tweets, 2022-06-29",jasoncrawford,3mo
1041,Correcting human error vs doing exactly what you're told - is there literature on this in context of general system design?Q,Jan Czechowski,3mo
1042,Latent Adversarial TrainingΩ,Adam Jermyn,3mo
1043,Limits to Legibility,Jan_Kulveit,3mo
1044,Will Capabilities Generalise More?Ω,Ramana Kumar,3mo
1045,"Kevin Kelly's ""103 Bits of Advice,"" Expanded",Dalton Mabery,3mo
1046,The table of different sampling assumptions in anthropics,avturchin,3mo
1047,Can We Align AI by Having It Learn Human Preferences? I’m Scared (summary  of last third of Human Compatible),apollonianblues,3mo
1048,Kurzgesagt – The Last Human (Youtube),habryka,3mo
1049,Game Review: This Merchant Life,Zvi,3mo
1050,Literature on How to Maximize PreferencesQ,josh,3mo
1051,Challenge: A Much More Alien Message,kman,3mo
1052,It’s Probably Not Lithium,Natália Coelho Mendonça,3mo
1053,"Reflections on Living in ""Guess Culture""",Dalton Mabery,3mo
1054,What Are You Tracking In Your Head?,johnswentworth,3mo
1055,Why is so much political commentary misleading?,contrarianbrit,3mo
1056,CFAR Handbook: Introduction,CFAR!Duncan,3mo
1057,Units of Exchange,CFAR!Duncan,3mo
1058,Scott Aaronson and Steven Pinker Debate AI Scaling,Liron,3mo
1059,A physicist's approach to Origins of Life,pchvykov,3mo
1060,What success looks likeΩ,"Marius Hobbhahn, MaxRa, JasperGeh, Yannick_Muehlhaeuser",3mo
1061,Four reasons I find AI safety emotionally compelling,"KatWoods, AmberDawn",3mo
1062,Some alternative AI safety research projectsΩ,Michele Campolo,3mo
1063,Doom doubts - is inner alignment a likely problem?,Crissman,3mo
1064,What is the LessWrong Logo(?) Supposed to Represent?Q,DragonGod,3mo
1065,Low-Friction MBTA Predictions,jefftk,3mo
1066,[Yann Lecun] A Path Towards Autonomous Machine Intelligence ,DragonGod,3mo
1067,Exploring Mild Behaviour in Embedded AgentsΩ,Megan Kinniment,3mo
1068,Epistemic modesty and how I think about AI risk,Aryeh Englander,3mo
1069,Deliberation Everywhere: Simple ExamplesΩ,Oliver Sourbut,3mo
1070,"Deliberation, Reactions, and Control: Tentative Definitions and a Restatement of Instrumental ConvergenceΩ",Oliver Sourbut,3mo
1071,Are long-form dating profiles productive?Q,"AABoyles, Lukas_Gloor",3mo
1072,Custom iPhone Widget to Encourage Less Wrong Use,Will Payne,3mo
1073,Announcing the Inverse Scaling Prize ($250k Prize Pool)Ω,"Ethan Perez, Ian McKenzie, Sam Bowman",3mo
1074,Air Conditioner Repair,Zvi,3mo
1075,Contest: An Alien Message,DaemonicSigil,3mo
1076,"Robin Hanson asks ""Why Not Wait On AI Risk?""",Gunnar_Zarncke,3mo
1077,Limits of Bodily Autonomy,jefftk,3mo
1078,Systems Biology for self studyQ,Ulisse Mini,3mo
1079,Announcing Epoch: A research organization investigating the road to Transformative AIΩ,"Jsevillamol, Pablo Villalobos, Tamay, lennart, Marius Hobbhahn, anson.ho",3mo
1080,Why Are Posts in the Sequences Tagged [Personal Blog] Instead of [Frontpage]?Q,DragonGod,3mo
1081,"Do You Care Whether There Are ""Successful"" Rationalists?",Matt Goldwater,3mo
1082,Training Trace Priors and Speed PriorsΩ,Adam Jermyn,3mo
1083,My current take on Internal Family Systems “parts”,Kaj_Sotala,3mo
1084,A Quick Ontology of Agreement,ravedon,3mo
1085,Seven ways to become unstoppably agentic,Evie Cottrell,3mo
1086,Formalizing Deception,JamesH,3mo
1087,Dust Theory vs Ruliad,svemirski,3mo
1088,How do poor countries get rich: some theories ,NathanBarnard,3mo
1089,The Basics of AGI Policy (Flowchart),Trevor1,3mo
1090,My cognitive inertia cycle,MSRayne,3mo
1091,Child Contracting,jefftk,3mo
1092,Conditioning Generative ModelsΩ,Adam Jermyn,3mo
1093,Unforgivable,Novalis,3mo
1094,SunPJ in Alenia,FlorianH,3mo
1095,Everything you need to know about UFOs (in 26 words),Trevor1,3mo
1096,Fundamental Uncertainty: Chapter 3 - Why can't we all agree on what's right?,G Gordon Worley III,3mo
1097,"How ""should"" counterfactual prediction markets work?Q",eapi,3mo
1098,Conversation with Eliezer: What do you want the system to do?,Akash,3mo
1099,AI-Written Critiques Help Humans Notice FlawsΩ,paulfchristiano,3mo
1100,Identification of Natural Modularity,Stephen Fowler,3mo
1101,Quick Summaries of Two Papers on Kant and Game Theory,Erich_Grunewald,3mo
1102,"Do you consider your current, non-superhuman self aligned with “humanity” already?Q",Rana Dexsin,3mo
1103,Some reflections on the LW community after several months of active engagement,M. Y. Zuo,3mo
1104,"On The Spectrum, On The Guest List: (vii) The Marquee",party girl,3mo
1105,[LQ] Some Thoughts on Messaging Around AI Risk,DragonGod,3mo
1106,Dependencies for AGI pessimism,Yitz,3mo
1107,[Link] Childcare : what the science says,Gunnar_Zarncke,3mo
1108,What if the best path for a person who wants to work on AGI alignment is to join Facebook or Google?,dbasch,3mo
1109,"[Link] Adversarially trained neural representations may already be as robust as
corresponding biological neural representations",Gunnar_Zarncke,3mo
1110,Updated Deference is not a strong argument against the utility uncertainty approach to alignmentΩ,Ivan Vendrov,3mo
1111,"Cracks in the Wall, Part I: The Conscious",silo,3mo
1112,Do alignment concerns extend to powerful non-AI agents?Q,Ozyrus,3mo
1113,Worked Examples of Shapley Values,lalaithion,3mo
1114,Intelligence in Commitment Races,David Udell,3mo
1115,Linkpost: Robin Hanson - Why Not Wait On AI Risk?,Yair Halberstadt,3mo
1116,LessWrong Has Agree/Disagree Voting On All New Comment Threads,Ben Pace,3mo
1117,How large of an army could you make with the first 'human-level' AGIs?,Josh,3mo
1118,Raphaël Millière on Generalization and Scaling Maximalism,Michaël Trazzi,3mo
1119,Feature request: voting buttons at the bottom?,Oliver Sourbut,3mo
1120,Does Lesswrong allow GIFs?,Trevor1,3mo
1121,Book review: The Passenger by Lisa Lutz,KatjaGrace,3mo
1122,20 Critiques of AI Safety That I Found on Twitter,dkirmani,3mo
1123,The Limits of Automation,milkandcigarettes,3mo
1124,Is CIRL a promising agenda?Q,Chris_Leong,3mo
1125,[Link] OpenAI: Learning to Play Minecraft with Video PreTraining (VPT),Aryeh Englander,3mo
1126,Half-baked AI Safety ideas thread,Aryeh Englander,3mo
1127,Nonprofit Boards are Weird,HoldenKarnofsky,3mo
1128,How do states respond to changes in nuclear risk,NathanBarnard,3mo
1129,What’s the contingency plan if we get AGI tomorrow?Q,"Yitz, Quintin Pope",3mo
1130,"What are the best ""policy"" approaches in worlds where alignment is difficult? Q",LHA,3mo
1131,AI Training Should Allow Opt-Out,alyssavance,3mo
1132,Loose thoughts on AGI risk,Yitz,3mo
1133,Covid 6/23/22: Under Five Alive,Zvi,3mo
1134,Air Conditioner Test Results & Discussion,johnswentworth,3mo
1135,Announcing the LessWrong Curated Podcast,"Ben Pace, Solenoid_Entity",3mo
1136,"Google's new text-to-image model - Parti, a demonstration of scaling benefits",Kayden,3mo
1137,Building an Epistemic Status Tracker,rcu,3mo
1138,Confusion about neuroscience/cognitive science as a danger for AI Alignment,Samuel Nellessen,3mo
1139,How do I use caffeine optimally?Q,randomstring,3mo
1140,Make learning a reality,Dalton Mabery,3mo
1141,Reflection Mechanisms as an Alignment target: A surveyΩ,"Marius Hobbhahn, elandgre, Beth Barnes",3mo
1142,How to Visualize Bayesianism,David Udell,3mo
1143,Are there spaces for extremely short-form rationality content?Q,Aleksi Liimatainen,3mo
1144,Solstice Movie Review: Summer Wars,JohnBuridan,3mo
1145,Security Mindset: Lessons from 20+ years of Software Security Failures Relevant to AGI Alignment,elspood,3mo
1146,A Quick List of Some Problems in AI Alignment As A Field,NicholasKross,3mo
1147,Politics is the Mind Killer. But is it getting worse?Q,Trevor1,3mo
1148,House Phone,jefftk,3mo
1149,What is the difference between AI misalignment and bad programming?Q,puzzleGuzzle,3mo
1150,One single meme that makes you Less Wrong in general,Trevor1,3mo
1151,What I mean by the phrase “getting intimate with reality”,Luise,3mo
1152,"What I mean by the phrase ""taking ideas seriously""",Luise,3mo
1153,"Progress links and tweets, 2022-06-20",jasoncrawford,3mo
1154,Debating Whether AI is Conscious Is A Distraction from Real Problems,sidhe_they,3mo
1155,Mitigating the damage from unaligned ASI by cooperating with aliens that don't exist yet,MSRayne,3mo
1156,The inordinately slow spread of good AGI conversations in ML,Rob Bensinger,3mo
1157,Getting from an unaligned AGI to an aligned AGI? Ω,Tor Økland Barstad,3mo
1158,Dagger of Detect Evil,lsusr,3mo
1159,What is the most probable AI?Q,Zeruel017,3mo
1160,Hydrophobic Glasses Coating Review,jefftk,3mo
1161,Common but neglected risk factors that may let you get Paxlovid,AllAmericanBreakfast,3mo
1162,How easy/fast is it for a AGI to hack computers/a human brain?Q,Noosphere89,3mo
1163,A Toy Model of Gradient HackingΩ,Oam Patel,3mo
1164,"Is This Thing Sentient, Y/N?",Thane Ruthenis,3mo
1165,Steam,abramdemski,3mo
1166,Parable: The Bomb that doesn't Explode,Conor Sullivan,3mo
1167,On corrigibility and its basinΩ,Donald Hobson,3mo
1168,Key Papers in Language Model Safety,aogara,3mo
1169,Relationship Advice Repository,Ruby,3mo
1170,Adaptation Executors and the Telos Margin,Plinthist,3mo
1171,Are we there yet?,theflowerpot,3mo
1172,Causal confusion as an argument against the scaling hypothesisΩ,RobertKirk,3mo
1173,An AI defense-offense symmetry thesis,Chris van Merwijk,3mo
1174,Evaluating a Corsi-Rosenthal Filter Cube,jefftk,3mo
1175,Survey re AIS/LTism office in NYC,RyanCarey,3mo
1176,Announcing the DWATV Discord,Zvi,3mo
1177,Let's See You Write That Corrigibility TagΩ,Eliezer Yudkowsky,3mo
1178,Half-baked alignment idea: training to generalize,Aaron Bergman,3mo
1179,Where I agree and disagree with EliezerΩ,paulfchristiano,3mo
1180,AI misalignment risk from GPT-like systems?Q,fiso64,3mo
1181,[Link-post] On Deference and Yudkowsky's AI Risk Estimates,bmg,3mo
1182,Have The Effective Altruists And Rationalists Brainwashed Me?,Matt Goldwater,3mo
1183,Hebbian Learning Is More Common Than You Think,Aleksi Liimatainen,3mo
1184,The Malthusian Trap: An Extremely Short Introduction,Davis Kedrosky,3mo
1185,Parliaments without the Parties,Yair Halberstadt,3mo
1186,Lamda is not an LLM,Kevin,3mo
1187,Getting stuck in local minima,louis030195,3mo
1188,[Linkpost] The importance of stupidity in scientific research,Pattern,3mo
1189,Specific problems with specific animal comparisons for AI policy,Trevor1,3mo
1190,ETH is probably undervalued right now,mukashi,3mo
1191,Juneberry Cake,jefftk,3mo
1192,Agent level parallelismΩ,Johannes C. Mayer,3mo
1193,What are our outs to play to?,Hastings,3mo
1194,What's the information value of government hearings?Q,Kenny,3mo
1195,What's the name of this fallacy/reasoning antipattern?Q,David Gross,3mo
1196,"""Brain enthusiasts"" in AI Safety","Jan, Samuel Nellessen",3mo
1197,To what extent have ideas and scientific discoveries gotten harder to find?,lsusr,3mo
1198,What's the goal in life?Q,Konstantin Weitz,3mo
1199,In defence of flailing,acylhalide,3mo
1200,Can DALL-E understand simple geometry?,Isaac King,3mo
1201,Why don't we think we're in the simplest universe with intelligent life?Q,ADifferentAnonymous,3mo
1202,Do yourself a FAVAR: security mindsetΩ,lcmgcd,3mo
1203,Forecasting Fusion Power,Daniel Kokotajlo,3mo
1204,Pivotal outcomes and pivotal processesΩ,Andrew_Critch,3mo
1205,The best 'free solo' (rock climbing) video,Kenny,3mo
1206,Scott Aaronson is joining OpenAI to work on AI safety,peterbarnett,3mo
1207,Quantifying General IntelligenceΩ,JasonBrown,3mo
1208,Things That Make Me Enjoy Giving Career Advice,Neel Nanda,3mo
1209,The Unified Theory of Normative Ethics,Thane Ruthenis,3mo
1210,1689: Uncovering the World New Institutionalism Created,Davis Kedrosky,3mo
1211,Is there an unified way to make sense of ai failure modes?Q,walking_mushroom,3mo
1212,"In defense of flailing, with foreword by Bill Burr",lc,3mo
1213,An Approach to Land Value Taxation,harsimony,3mo
1214,Value extrapolation vs WireheadingΩ,Stuart_Armstrong,3mo
1215,#SAT with Tensor Networks,Adam Jermyn,3mo
1216,Is civilizational alignment on the table?Q,Aleksi Liimatainen,3mo
1217,wrapper-minds are the enemyΩ,nostalgebraist,3mo
1218,A Litany Missing from the Canon,benwr,3mo
1219,Apply for Productivity Coaching and AI Alignment Mentorship,Nick,3mo
1220,Announcing the Clearer Thinking Regrants program,spencerg,3mo
1221,Apply to the Machine Learning For Good bootcamp in France,Alexandre Variengien,3mo
1222,What's it like to have sex with Duncan?,Duncan_Sabien,3mo
1223,BBC Future covers progress studies,jasoncrawford,3mo
1224,Humans are very reliable agentsΩ,alyssavance,2mo
1225,Towards Gears-Level Understanding of Agency,Thane Ruthenis,3mo
1226,"A possible AI-inoculation due to early ""robot uprising""",shminux,3mo
1227,"[Link] ""The madness of reduced medical diagnostics"" by Dynomight",Kenny,3mo
1228,Breaking Down Goal-Directed BehaviourΩ,Oliver Sourbut,3mo
1229,Perils of optimizing in social contexts,owencb,3mo
1230,Don't Over-Optimize Things,owencb,3mo
1231,Security analysis of 'cloud chemistry labs'?Q,Kenny,3mo
1232,Is there a worked example of Georgian taxes?Q,Dagon,3mo
1233,"Alignment research for ""meta"" purposes",acylhalide,3mo
1234,"Ten experiments in modularity, which we'd like you to run!Ω","TheMcDouglas, Lucius Bushnaq, Avery",3mo
1235,What if LaMDA is indeed sentient / self-aware / worth having rights?Q,"RomanS, Daniel Kokotajlo",3mo
1236,"AI Risk, as Seen on Snapchat",dkirmani,3mo
1237,Covid 6/16/22: Do Not Hand it to Them,Zvi,3mo
1238,Against Active Shooter Drills,Zvi,3mo
1239,Contra Hofstadter on GPT-3 Nonsense,rictic,3mo
1240,Optimization power as divergence from default trajectories,Josh,3mo
1241,"Progress links and tweets, 2022-06-13",jasoncrawford,3mo
1242,Contextual Evil,ACrackedPot,3mo
1243,Multigate Priors,Adam Jermyn,3mo
1244,FYI: I’m working on a book about the threat of AGI/ASI for a general audience. I hope it will be of value to the cause and the community,Darren McKee,3mo
1245,What are all the AI Alignment and AI Safety Communication Hubs?Q,Gunnar_Zarncke,3mo
1246,"Georgism, in theory",Stuart_Armstrong,3mo
1247,"A central AI alignment problem: capabilities generalization, and the sharp left turnΩ",So8res,3mo
1248,Our mental building blocks are more different than I thought,Marius Hobbhahn,3mo
1249,Has there been any work on attempting to use Pascal's Mugging to make an AGI behave?Q,Chris_Leong,3mo
1250,High Powers Over Physics,DragonGod,3mo
1251,Alignment Risk Doesn't Require Superintelligence,JustisMills,3mo
1252,A Butterfly's View of Probability,Gabriel Wu,3mo
1253,Favourite new AI productivity tools?Q,Gabriel Mukobi,3mo
1254,I applied for a MIRI job in 2020. Here's what happened next.,ViktoriaMalyasova,3mo
1255,"Yes, AI research will be substantially curtailed if a lab causes a major disaster",lc,3mo
1256,Slow motion videos as AI risk intuition pumps,Andrew_Critch,3mo
1257,Cryptographic Life: How to transcend in a sub-lightspeed world via Homomorphic encryption,Golol,3mo
1258,Blake Richards on Why he is Skeptical of Existential Risk from AI,Michaël Trazzi,3mo
1259,How Do You Quantify [Physics Interfacing] Real World Capabilities?Q,DragonGod,3mo
1260,Was the Industrial Revolution The Industrial Revolution?,Davis Kedrosky,3mo
1261,Investigating causal understanding in LLMsΩ,"Marius Hobbhahn, Tom Lieberum",3mo
1262,Why multi-agent safety is important ,Akbir Khan,3mo
1263,Resources I send to AI researchers about AI safety,Vael Gates,3mo
1264,Vael Gates: Risks from Advanced AI (June 2022),Vael Gates,3mo
1265,"OpenAI: GPT-based LLMs show ability to discriminate between its own wrong answers, but inability to explain how/why it makes that discrimination, even as model scales",Aditya Jain,3mo
1266,Was Eliezer Yudkowsky right to give himself 10% to succeed with HPMoR in 2010?Q,amaury lorin,3mo
1267,Continuity AssumptionsΩ,Jan_Kulveit,3mo
1268,Crypto-fed Computation,aaguirre,3mo
1269,Contra EY: Can AGI destroy us without trial & error?,Nikita Sokolsky,3mo
1270,[Link] New SEP article on Bayesian Epistemology,Aryeh Englander,3mo
1271,Training Trace PriorsΩ,Adam Jermyn,3mo
1272,Can you MRI a deep learning model?Q,Yair Halberstadt,3mo
1273,On A List of Lethalities,Zvi,3mo
1274,D&D.Sci June 2022 Evaluation and Ruleset,abstractapplic,3mo
1275,"What's the ""This AI is of moral concern."" fire alarm?Q",Quintin Pope,3mo
1276,Code Quality and Rule Consequentialism,Adam Zerner,3mo
1277,"Who said something like ""The fact that putting 2 apples next to 2 other apples leads to there being 4 apples there has nothing to do with the fact that 2 + 2 = 4""?Q",hunterglenn,3mo
1278,What are some smaller-but-concrete challenges related to AI safety that are impacting people today?,nonzerosum,3mo
1279,The beautiful magical enchanted golden Dall-e Mini is underrated,p.b.,3mo
1280,Why so little AI risk on rationalist-adjacent blogs?,Grant Demaree,3mo
1281,Grokking “Semi-informative priors over AI timelines”,anson.ho,3mo
1282,How much does cybersecurity reduce AI risk?Q,Darmani,3mo
1283,How are compute assets distributed in the world?Q,Chris van Merwijk,3mo
1284,Intuitive Explanation of AIXI,Thomas Larsen,3mo
1285,Why all the fuss about recursive self-improvement?,So8res,3mo
1286,Why the Kaldor-Hicks criterion can be non-transitive,Rupert,3mo
1287,How do you post links here?Q,skybrian,3mo
1288,Pure Altruism,Matt Goldwater,3mo
1289,How To: A Workshop (or anything),Duncan_Sabien,3mo
1290,A claim that Google's LaMDA is sentient,Ben Livengood,3mo
1291,How much stupider than humans can AI be and still kill us all through sheer numbers and resource access?Q,shminux,3mo
1292,Let's not name specific AI labs in an adversarial context,acylhalide,3mo
1293,Filter out tags from the front page?Q,"jaspax, Yoav Ravid",3mo
1294,ELK Proposal - Make the Reporter care about the Predictor’s beliefsΩ,"Adam Jermyn, Nicholas Schiefer",3mo
1295,Why has no person / group ever taken over the world? Q,"Aryeh Englander, jimrandomh",3mo
1296,Poorly-Aimed Death Rays,Thane Ruthenis,3mo
1297,AGI Safety Communications Initiative,ines,3mo
1298,Why don't you introduce really impressive people you personally know to AI alignment (more often)?Q,"Verden, outerloper",3mo
1299,Godzilla StrategiesΩ,johnswentworth,3mo
1300,Steganography and the CycleGAN - alignment failure case study,Jan Czechowski,3mo
1301,The Mountain Troll,lsusr,3mo
1302,Show LW: YodaTimer.com,Adam Zerner,3mo
1303,How fast can we perform a forward pass?,jsteinhardt,3mo
1304,Are there English-speaking meetups in Frankfurt/Munich/Zurich?Q,Grant Demaree,3mo
1305,Beauty and the Beast,Tomás B.,3mo
1306,A gaming group for rationality-aware people,dhatas,3mo
1307,"Summary of ""AGI Ruin: A List of Lethalities""",Stephen McAleese,3mo
1308,How dangerous is human-level AI?,Alex_Altair,3mo
1309,"Another plausible scenario of AI risk: AI builds military infrastructure while collaborating with humans, defects later.",avturchin,3mo
1310,"Leaving Google, Joining the Nucleic Acid Observatory",jefftk,3mo
1311,Progress Report 6: get the tool working,Nathan Helm-Burger,4mo
1312,"I No Longer Believe Intelligence to be ""Magical""",DragonGod,4mo
1313,[linkpost] The final AI benchmark: BIG-bench,RomanS,4mo
1314,Could Patent-Trolling delay AI timelines?Q,Pablo Repetto,4mo
1315,Kolmogorov's AI ForecastQ,interstice,4mo
1316,"Tao, Kontsevich & others on HLAI in Math",interstice,4mo
1317,A plausible story about AI risk.,DeLesley Hutchins,4mo
1318,Open Problems in AI X-Risk [PAIS #5]Ω,"Dan Hendrycks, ThomasW",4mo
1319,why assume AGIs will optimize for fixed goals?QΩ,"nostalgebraist, Rob Bensinger",4mo
1320,Bureaucracy of AIs,Logan Zoellner,4mo
1321,"On The Spectrum, On The Guest List: (v) The Fleur Room",party girl,3mo
1322,Is AI Alignment Impossible?Q,Heighn,4mo
1323,You Only Get One Shot: an Intuition Pump for Embedded AgencyΩ,Oliver Sourbut,4mo
1324,Forestalling Atmospheric IgnitionQ,Conor Sullivan,4mo
1325,How Do Selection Theorems Relate To Interpretability?Ω,johnswentworth,4mo
1326,"Progress links and tweets, 2022-06-08",jasoncrawford,4mo
1327,"If no near-term alignment strategy, research should aim for the long-term ",harsimony,4mo
1328,Operationalizing two tasks in Gary Marcus’s AGI challenge,Bill Benzon,4mo
1329,Why it's bad to kill Grandma,dynomight,4mo
1330,Modeling humanity's robustness to GCRs?Q,rodeo_flagellum,4mo
1331,"If there was a millennium equivalent prize for AI alignment, what would the problems be?Q",Yair Halberstadt,4mo
1332,Book Review: How the World Became Rich,Davis Kedrosky,4mo
1333,Website For Yoda Timers,Adam Zerner,4mo
1334,AI Could Defeat All Of Us Combined,HoldenKarnofsky,4mo
1335,The “mind-body vicious cycle” model of RSI & back pain,Steven Byrnes,4mo
1336,[Linkpost & Discussion] AI Trained on 4Chan Becomes ‘Hate Speech Machine’ [and outperforms GPT-3 on TruthfulQA Benchmark?!],Yitz,4mo
1337,Covid 6/9/22: Nice,Zvi,4mo
1338,"Comment reply: my low-quality thoughts on why CFAR didn't get farther with a ""real/efficacious art of rationality""",AnnaSalamon,4mo
1339,Entitlement as a major amplifier of unhappiness,VipulNaik,4mo
1340,Silly Online RulesQ,Gunnar_Zarncke,4mo
1341,"Untypical SIA

",avturchin,4mo
1342,Eliciting Latent Knowledge (ELK) - Distillation/SummaryΩ,Marius Hobbhahn,4mo
1343,Transformer Research Questions from Stained Glass Windows,StefanHex,4mo
1344,Why I don't believe in doom,mukashi,4mo
1345,Has anyone actually tried to convince Terry Tao or other top mathematicians to work on alignment?Q,"P., TekhneMakre",4mo
1346,Steelmanning Marxism/CommunismQ,Suh_Prance_Alot,4mo
1347,Staying Split: Sabatini and Social Justice,Duncan_Sabien,4mo
1348,Puddle Temperature Alarm,jefftk,4mo
1349,"""Pivotal Acts"" means something specific",Raemon,4mo
1350,Embodiment is Indispensable for AGI,P. G. Keerthana Gopalakrishnan,4mo
1351,Stephen Wolfram's ideas are under-appreciated,Kenny,4mo
1352,Who models the models that model models? An exploration of GPT-3's in-context model fitting abilityΩ,Lovre,4mo
1353,How Does Cognitive Performance Translate to Real World Capability?Q,DragonGod,4mo
1354,"On The Spectrum, On The Guest List: (iv) Silencio",party girl,4mo
1355,Confused Thoughts on AI Afterlife (seriously)Q,Epirito,4mo
1356,Thinking about Broad Classes of Utility-like Functions,Jemist,4mo
1357,Thoughts on Formalizing Composition,Tom Lieberum,4mo
1358,AGI Safety FAQ / all-dumb-questions-allowed thread,Aryeh Englander,4mo
1359,Pitching an Alignment Softball,mu_(negative),4mo
1360,We will be around in 30 years,mukashi,4mo
1361,Where to Live for Happiness,ethanmorse,4mo
1362,"A descriptive, not prescriptive, overview of current AI Alignment ResearchΩ","Jan, Logan Riggs, jacquesthibs, janus",4mo
1363,We haven't quit evolution [short],the gears to ascenscion,4mo
1364,Grokking “Forecasting TAI with biological anchors”Ω,anson.ho,4mo
1365,"DALL-E 2 - Unofficial Natural Language Image Editing, Art Critique Survey",bakztfuture,4mo
1366,Health & Lifestyle Interventions With Heavy-Tailed Outcomes?Q,MondSemmel,4mo
1367,Reading the ethicists 2: Hunting for AI alignment papersΩ,Charlie Steiner,4mo
1368,Some ideas for follow-up projects to Redwood Research’s recent paperΩ,JanBrauner,4mo
1369,Give the model a model-builder,Adam Jermyn,4mo
1370,What journaling prompts do you use?Q,ChristianKl,4mo
1371,Why do some people try to make AGI?,TekhneMakre,4mo
1372,Miriam Yevick on why both symbols and networks are necessary for artificial minds,Bill Benzon,4mo
1373,Optimization and Adequacy in Five Bullets,james.lucassen,4mo
1374,Microphone on Electric Mandolin II,jefftk,4mo
1375,We Should Break Up Elite Colleges,sahajsharda,4mo
1376,What board games would you recommend?Q,Yair Halberstadt,4mo
1377,Transcript of a Twitter Discussion on EA from June 2022,Zvi,4mo
1378,AGI Ruin: A List of LethalitiesΩ,Eliezer Yudkowsky,4mo
1379,Noisy environment regulate utility maximizers,Niclas Kupper,4mo
1380,Expected Value vs. Expected Growth,tom-pollak,4mo
1381,New cooperation mechanism - quadratic funding without a matching pool,Filip Sondej,4mo
1382,The Fourth Arena 2: New beings in time,Bill Benzon,4mo
1383,"Russian x-risks newsletter May 2022 + short history of ""methodologists""",avturchin,4mo
1384,Some questions I've been pondering...,Dalton Mabery,4mo
1385,Teaching all things to all people in all ways,Anonymous,4mo
1386,Northboro Air Quality Measurement,jefftk,4mo
1387,Reinventing the wheel,jasoncrawford,4mo
1388,What do you do to deliberately practice?,hath,4mo
1389,Quick Look: Asymptomatic Herpes Shedding,Elizabeth,4mo
1390,How to pursue a career in technical AI alignment,charlie.rs,4mo
1391,"The Fourth Arena: What’s Up in the world these days? We’re moving to a new, a new what? ",Bill Benzon,4mo
1392,Towards a Formalisation of Returns on Cognitive Reinvestment (Part 1),DragonGod,4mo
1393,Does quantum mechanics predict all the effects of relativity except gravity?Q,"EniScien, Razied",4mo
1394,Deep Learning Systems Are Not Less Interpretable Than Logic/Probability/EtcΩ,johnswentworth,4mo
1395,Announcing the Alignment of Complex Systems Research GroupΩ,"Jan_Kulveit, technicalities",4mo
1396,Eternal youth as eternal suffering,shminux,4mo
1397,"D&D.Sci June 2022: A Goddess Tried To Reincarnate Me Into A Fantasy World, But I Insisted On Using Data Science To Select An Optimal Combination Of Cheat Skills!",abstractapplic,4mo
1398,The Burden of Worldbuilding,lsusr,4mo
1399,Longtermist Consequences of a New Dark Age?Q,David Udell,4mo
1400,Open & Welcome Thread - June 2022 ,MondSemmel,4mo
1401,Just Say No to Utilitarianism,Arjun Panickssery,4mo
1402,The STEM Attractor,David Udell,4mo
1403,Book Review: Talent,Zvi,4mo
1404,"On The Spectrum, On The Guest List: (iii) Etiquette",party girl,4mo
1405,Give the AI safe tools,Adam Jermyn,4mo
1406,"I'm trying out ""asteroid mindset""",Alex_Altair,4mo
1407,Monthly Shorts 5/22,Celer,4mo
1408,"On effective altruism, utilitarianism and localness",David Hugh-Jones,4mo
1409,[Linkpost] A Chinese AI optimized for killing,RomanS,4mo
1410,Intergenerational trauma impeding cooperative existential safety efforts,Andrew_Critch,4mo
1411,Silliness,lsusr,4mo
1412,"[MLSN #4]: Many New Interpretability Papers, Virtual Logit Matching, Rationalization Helps RobustnessΩ",Dan Hendrycks,4mo
1413,A short conceptual explainer of Immanuel Kant's Critique of Pure Reason,jessicata,4mo
1414,"Adversarial training, importance sampling, and anti-adversarial training for AI whistleblowingΩ",Buck,4mo
1415,Rationality camps in Oxford for 16-20 yo: Apply by June 12th,Anna Gajdova,4mo
1416,Another Calming Example,jefftk,4mo
1417,"Confused why a ""capabilities research is good for alignment progress"" position isn't discussed moreΩ",Kaj_Sotala,4mo
1418,Announcing a contest: EA Criticism and Red Teaming,fin,4mo
1419,Fact post: project-based learning,dominicq,4mo
1420,The case for using the term 'steelmanning' instead of 'principle of charity',ChristianKl,4mo
1421,"The horror of what must, yet cannot, be true",Kaj_Sotala,4mo
1422,Paradigms of AI alignment: components and enablersΩ,Vika,4mo
1423,The Bio Anchors Forecast,Ansh Radhakrishnan,4mo
1424,Covid 6/2/22: Declining to Respond,Zvi,4mo
1425,Public beliefs vs. Private beliefs,Eli Tyre,4mo
1426,Probability that the President would win election against a random adult citizen?Q,"Daniel Kokotajlo, Zvi",4mo
1427,"Revisiting ""Why Global Poverty""",jefftk,4mo
1428,What will happen when an all-reaching AGI starts attempting to fix human character flaws?Q,Michael Bright,4mo
1429,Any prior work on mutiagent dynamics for continuous distributions over agents?Q,Quintin Pope,4mo
1430,[Repost] Non-Nashian Game Theory: A Normal-Form Primer,Ghislain Fourny,4mo
1431,Machines vs Memes Part 3: Imitation and Memes,ceru23,4mo
1432,Rationalism in an Age of Egregores,David Udell,4mo
1433,Wielding civilization,dominicq,4mo
1434,Artificial Intelligence Safety for the Averagely Intelligent,Salsabila Mahdi,4mo
1435,Formation via nucleation of boltzmann brainsQ,Zeruel017,4mo
1436,Machines vs. Memes 2: Memetically-Motivated Model Extensions,naterush,4mo
1437,Machines vs Memes Part 1: AI Alignment and Memetics,Harriet Farlow,4mo
1438,The Hard Intelligence Hypothesis and Its Bearing on Succession Induced Foom,DragonGod,4mo
1439,Paper: Teaching GPT3 to express uncertainty in wordsΩ,Owain_Evans,4mo
1440,What is the state of Chinese AI research?Q,Ratios,4mo
1441,The Brain That Builds Itself,Jan,4mo
1442,Is there any formal argument that climate change needs to more extreme weather events?Q,"ChristianKl, jbash",4mo
1443,"Progress links and tweets, 2022-05-30",jasoncrawford,4mo
1444,The Reverse Basilisk,Dunning K.,4mo
1445,Deliberate Grieving,Raemon,1mo
1446,Perform Tractable Research While Avoiding Capabilities Externalities [Pragmatic AI Safety #4]Ω,"Dan Hendrycks, ThomasW",4mo
1447,A terrifying variant of Boltzmann's brains problemQ,Zeruel017,4mo
1448,Ceiling Air Purifier,jefftk,4mo
1449,Notion template for personal predictions,Arjun Yadav,4mo
1450,Six Dimensions of Operational Adequacy in AGI ProjectsΩ,Eliezer Yudkowsky,3mo
1451,My SERI MATS Application,Daniel Paleka,4mo
1452,Reshaping the AI IndustryΩ,Thane Ruthenis,4mo
1453,The Unbearable Lightness of Web Vulnerabilities,aiiixiii,4mo
1454,Finding the Right Problem,tobot,4mo
1455,The impact you might have working on AI safety,Fabien,4mo
1456,The Problem With The Current State of AGI DefinitionsΩ,Yitz,4mo
1457,Request for nice questions to think about while trying to sleepQ,oh54321,4mo
1458,Will working here advance AGI? Help us not destroy the world!,Yonatan Cale,4mo
1459,Passable Puppet,burmesetheater,4mo
1460,"Multiple AIs in boxes, evaluating each other's alignment",Moebius314,4mo
1461,How would you build Dath Ilan on earth?Q,"Yair Halberstadt, dkirmani",4mo
1462,Distributed DecisionsΩ,johnswentworth,4mo
1463,Distilled - AGI Safety from First Principles,Harrison G,4mo
1464,"What have been the major ""triumphs"" in the field of AI over the last ten years?Q",lc,4mo
1465,Utilitarianism,C S SRUTHI,4mo
1466,Bayesian Persuasion?Q,Karthik Tadepalli,4mo
1467,An inquiry into the thoughts of twenty-five people in India,KatjaGrace,4mo
1468,Understanding Selection Theorems,adamk,4mo
1469,What is Going On With CFAR?Q,"niplav, AnnaSalamon",4mo
1470,Functional Analysis Reading Group,Ulisse Mini,4mo
1471,"Infernal Corrigibility, Fiendishly Difficult",David Udell,4mo
1472,New Water Quality x Obesity Dataset Available,Elizabeth,4mo
1473,Bootstrapping Language Models,harsimony,4mo
1474,Sneaking Suspicion,benwr,4mo
1475,Range and Forecasting Accuracy,niplav,4mo
1476,"Croesus, Cerberus, and the magpies: a gentle introduction to Eliciting Latent Knowledge",Alexandre Variengien,4mo
1477,Deconfusing Landauer's Principle,euanmclean,4mo
1478,"On The Spectrum, On The Guest List: (ii) LAVO",party girl,4mo
1479,Can growth continue?,jasoncrawford,4mo
1480,An academic journal is just a Twitter feed,David Hugh-Jones,4mo
1481,Grabby Animals: Observation-selection effects favor the hypothesis that UAP are animals which consist of the “field-matter”: ,avturchin,4mo
1482,"Iterated Distillation-Amplification, Gato, and Proto-AGI [Re-Explained]",Gabriel Mukobi,4mo
1483,"Where Utopias Go Wrong, or: The Four Little Planets",ExCeph,4mo
1484,A Story of AI Risk: InstructGPT-N,peterbarnett,4mo
1485,Resources For the Redwood Research Triplebyte Test,Aiyen,4mo
1486,The Pointers Problem - Distilled,NinaR,4mo
1487,Infra-Bayesianism Distillation: Realizability and Decision Theory,Thomas Larsen,4mo
1488,"On inner and outer alignment, and their confusion",NinaR,4mo
1489,The misguided pursuit of existentialism and absurdism in combatting nihilism,Sundae,4mo
1490,Brass Puppet,abramdemski,4mo
1491,"A discussion of the paper, ""Large Language Models are Zero-Shot Reasoners""",HiroSakuraba,4mo
1492,"Moral Weights of Six Animals, Considering Viewpoint Uncertainty - Seeds of Science call for reviewers",rogersbacon,4mo
1493,CNN feature visualization in 50 lines of code,StefanHex,4mo
1494,How much white collar work could be automated using existing ML models?Q,AM,4mo
1495,Benign Boundary Violations,Duncan_Sabien,3mo
1496,Here's a List of Some of My Ideas for Blog Posts,lsusr,4mo
1497,View and bet in Manifold prediction markets on Lesswrong,Sinclair Chen,4mo
1498,"Right now, you're sitting on a REDONKULOUS opportunity to help solve AGI (and rake in $$$)",Trevor1,4mo
1499,Experience LessWrong without the Time-Wasting RabbitHole Effect,joraine,4mo
1500,Covid 5/26/22: I Guess I Should Respond To This Week’s Long Covid Study,Zvi,4mo
1501,Testing Air Purifiers,jefftk,4mo
1502,How to get people to produce more great exposition? Some strategies and their assumptions,riceissa,4mo
1503,Request for small textbook recommendationsQ,Alex_Altair,4mo
1504,Science-informed normativity,Richard_Ngo,4mo
1505,Reflections on a year of college,unoptimal,4mo
1506,"The ""Measuring Stick of Utility"" ProblemΩ",johnswentworth,4mo
1507,What's The Best Place to Look When You Have A Question About x?Q,Jalen Lyle-Holmes,4mo
1508,RL with KL penalties is better seen as Bayesian inferenceΩ,"Tomek Korbak, Ethan Perez",4mo
1509,A Quick Note on AI Scaling Asymptotes,alyssavance,4mo
1510,Visible Homelessness in SF: A Quick Breakdown of Causes,alyssavance,4mo
1511,autonomy: the missing AGI ingredient?Ω,nostalgebraist,4mo
1512,Explaining inner alignment to myself,Jeremy Gillen,4mo
1513,MERV Filters for Covid?,jefftk,4mo
1514,Google's Imagen uses larger text encoder,Ben Livengood,4mo
1515,"Endpoint Specification, or Good Help is Hard to Find",ACrackedPot,4mo
1516,Game Theory and Partner Abuse,arielleplus,4mo
1517,I only believe in the paranormal,Malmesbury,4mo
1518,The No Free Lunch theorems and their RazorΩ,Adrià Garriga-alonso,4mo
1519,Synthetic Media and The Future of Film,ifalpha,4mo
1520,Complex Systems for AI Safety [Pragmatic AI Safety #3]Ω,"Dan Hendrycks, ThomasW",4mo
1521,Mandatory Post About Monkeypox,Zvi,4mo
1522,Why I Define My Experience At the Monastic Academy As Sexual Assault,HS2021,4mo
1523,"Contra Astral Codex Ten's ""On Sexy In-Laws""",Dalton Mabery,4mo
1524,Why I'm Worried About AI,peterbarnett,4mo
1525,Bits of Optimization Can Only Be Lost Over A DistanceΩ,johnswentworth,4mo
1526,[Alignment] Is there a census on who's working on what?Q,Cedar,4mo
1527,Finding Your Voice,Evenflair,4mo
1528,Science for the Possible World,Zechen Zhang,4mo
1529,AXRP Episode 15 - Natural Abstractions with John WentworthΩ,DanielFilan,4mo
1530,PSA: The Sequences don't need to be read in sequence,kave,4mo
1531,Gradations of AgencyΩ,Daniel Kokotajlo,4mo
1532,[fiction]A Question of Perspective,Elias,4mo
1533,Monkeypox: explaining the jump to Europe,AllAmericanBreakfast,4mo
1534,How curing aging could help progress,jasoncrawford,4mo
1535,Adversarial attacks and optimal controlΩ,Jan,4mo
1536,My Take On Philosophy,Matt Goldwater,4mo
1537,What kinds of algorithms do multi-human imitators learn?,"Chris van Merwijk, Joar Skalse",4mo
1538,Are human imitators superhuman models with explicit constraints on capabilities?,Chris van Merwijk,4mo
1539,The value of x-risk reduction,NathanBarnard,4mo
1540,"On The Spectrum, On The Guest List",party girl,4mo
1541,An Agent Based Consciousness Model (unfortunately it's not computable),Logan Zoellner,4mo
1542,Help! What do I say at my wedding?Q,Jonathan Task,4mo
1543,Wormy the Worm,River Lewis,4mo
1544,Philosophical considerations of cessation of brain activity,clockwork,4mo
1545,[Short version] Information Loss --> Basin flatnessΩ,Vivek Hebbar,4mo
1546,Information Loss --> Basin flatnessΩ,Vivek Hebbar,4mo
1547,The Future of Nuclear War,avturchin,4mo
1548,Clarifying what ELK is trying to achieve,Simon Skade,4mo
1549,How Big Are Covid Particles?,jefftk,4mo
1550,[retracted] A really simplistic experiment for LessWrong and /r/SneerClub,NicholasKross,4mo
1551,"Who is available for contract work? (A la the Hacker News ""Who Wants To Be Hired"" threads)Q",nonzerosum,4mo
1552,Why does gradient descent always work on neural networks?Q,"MichaelDickens, lalaithion",4mo
1553,Beware boasting about non-existent forecasting track records,Jotto999,4mo
1554,How RL Agents Behave When Their Actions Are Modified? [Distillation post]Ω,PabloAMC,4mo
1555,Over-digitalization: A Prelude to Analogia (Chapter 6) ,Justin Bullock,4mo
1556,Podcast: Spencer Greenberg talks to me about dealing with our groupstruckness and boundedness,KatjaGrace,4mo
1557,Proclamation of game-theoretic emergence,dr_diagoras,4mo
1558,How has the total amount of gain-of-function research worldwide grown/shrunk over time?Q,johnswentworth,4mo
1559,14 Techniques to Accelerate Your Learning,"spencerg, Florence Hinder",4mo
1560,Formula for a Shortage,Zvi,4mo
1561,Covid 5/19/22: The Law of Five,Zvi,4mo
1562,Should you kiss it better?Q,Yair Halberstadt,4mo
1563,Looking for AI / Application Engineer @ OpenPrinciples -- Help people align actions with principles,ti_guo,4mo
1564,We have achieved Noob Gains in AIΩ,phdead,4mo
1565,A possible check against motivated reasoning using elicit.org,david reinstein,4mo
1566,Reading the ethicists: A review of articles on AI in the journal Science and Engineering Ethics,Charlie Steiner,4mo
1567,Maxent and Abstractions: Current Best ArgumentsΩ,johnswentworth,4mo
1568,Framing Practicum: Dynamic Programming,aysajan,4mo
1569,How to get into AI safety researchΩ,Stuart_Armstrong,4mo
1570,A bridge to Dath Ilan?  Improved governance on the critical path to AI alignment.,Jackson Wagner,4mo
1571,Understanding Gato's Supervised Reinforcement Learning,Lorenzo Rex,4mo
1572,Gato's Generalisation: Predictions and Experiments I'd Like to SeeΩ,Oliver Sourbut,4mo
1573,I just watched the Open C3 Subcommittee Hearing on Unidentified Aerial Phenomena (UFOs). Here's a succinct summary and commentary + some background,MakoYass,4mo
1574,Feminism and Femininity,NathanBarnard,4mo
1575,What's up with the recent monkeypox cases?Q,"johnswentworth, AllAmericanBreakfast",4mo
1576,Workday Air Quality Measurements,jefftk,4mo
1577,Popular education in Sweden: much more than you wanted to know,Henrik Karlsson,4mo
1578,On saving one's world,Rob Bensinger,4mo
1579,Proposal: Twitter dislike button,KatjaGrace,4mo
1580,Formula for Dying Babies,Zvi,4mo
1581,"[Intro to brain-like-AGI safety] 15. Conclusion: Open problems, how to help, AMAΩ",Steven Byrnes,4mo
1582,What are the possible trajectories of an AGI/ASI world?,Jakobovski,4mo
1583,"The ""Adults in the Room""",David Udell,4mo
1584,D&D.Sci Divination: Nine Black Doves Evaluation & Ruleset,aphyer,4mo
1585,Feature request: draft comments,AllAmericanBreakfast,4mo
1586,BERI is seeking new collaborators (2022),sawyer,4mo
1587,Actionable-guidance and roadmap recommendations for the NIST AI Risk Management FrameworkΩ,"Dan Hendrycks, Tony Barrett",4mo
1588,How to invest in expectation of AGI?,Jakobovski,4mo
1589,"DeepMind’s generalist AI, Gato: A non-technical explainer","frances_lorenz, Nora Belrose, jonmenaster",4mo
1590,[link] The Lion and the Worm,TsviBT,4mo
1591,"[Link] New Stanford Encyclopedia of Philosophy article, ""Normative Theories of Rational Choice: Rivals to Expected Utility""",Aryeh Englander,4mo
1592,Proxy misspecification and the capabilities vs. value learning raceΩ,Sam Marks,4mo
1593,Optimization at a DistanceΩ,johnswentworth,4mo
1594,"To what extent is your AGI timeline bimodal or otherwise ""bumpy""?Q",jchan,4mo
1595,The Cuban missile crisis: the strategic context,NathanBarnard,4mo
1596,Why I'm Optimistic About Near-Term AI Risk,harsimony,4mo
1597,China Covid #3,Zvi,4mo
1598,BIDA Air Quality Measurements,jefftk,4mo
1599,Definition Practice: Applied RationalityQ,ChristianKl,4mo
1600,Surviving Automation In The 21st Century - Part 1,George3d6,4mo
1601,The AI Countdown Clock,River Lewis,4mo
1602,Is AI Progress Impossible To Predict?,alyssavance,4mo
1603,My Morality,Matt Goldwater,4mo
1604,Gato as the Dawn of Early AGI,David Udell,4mo
1605,Should we buy Google stock?Q,Sergio Manuel Justo Maceda,4mo
1606,Is it possible to implement switching between sequences from its pages?Q,EniScien,4mo
1607,[Link post] Promising Paths to Alignment - Connor Leahy | Talk,frances_lorenz,4mo
1608,Inequality is inseparable from markets ,NathanBarnard,4mo
1609,Predicting the Elections with Deep Learning - Part 1 - Results,Quentin Chenevier,4mo
1610,Clarifying the confusion around inner alignmentΩ,Rauno Arike,4mo
1611,What's up with the font size in the Markdown text editor?Q,Ege Erdil,4mo
1612,Costs and benefits of amniocentesis for normal pregnancies,braces,4mo
1613,Frame for Take-Off Speeds to inform compute governance & scaling alignmentΩ,Logan Riggs,4mo
1614,Alignment as ConstraintsΩ,Logan Riggs,4mo
1615,How close to nuclear war did we get over Cuba?,NathanBarnard,4mo
1616,Against Time in Agent ModelsΩ,johnswentworth,4mo
1617,Agency As a Natural AbstractionΩ,Thane Ruthenis,4mo
1618,Fermi estimation of the impact you might have working on AI safety,Fabien,4mo
1619,"""Tech company singularities"", and steering them to reduce x-risk",Andrew_Critch,4mo
1620,An observation about Hubinger et al.'s framework for learned optimizationΩ,Spencer Becker-Kahn,4mo
1621,The Economics of a New Energy Source Q,hatta_afiq,4mo
1622,Thoughts on AI Safety Camp,Charlie Steiner,4mo
1623,Deferring,owencb,4mo
1624,Still possible to change username?Q,gabrielrecc,4mo
1625,"[Rough notes, BAIS] Human values and cyclical preferences","pranomostro, Jayjay, Lucie Philippon",4mo
1626,Can moderators fix old sequences posts?Q,EniScien,4mo
1627,DeepMind is hiring for the Scalable Alignment and Alignment TeamsΩ,"Rohin Shah, Geoffrey Irving",4mo
1628,RLHF,Ansh Radhakrishnan,4mo
1629,What to do when starting a business in an imminent-AGI world?Q,ryan_b,4mo
1630,Interpretability’s Alignment-Solving Potential: Analysis of 7 ScenariosΩ,Evan R. Murphy,4mo
1631,Introduction to the sequence: Interpretability Research for the Most Important CenturyΩ,Evan R. Murphy,4mo
1632,A tentative dialogue with a Friendly-boxed-super-AGI on brain uploads,Ramiro P.,4mo
1633,The Last Paperclip,Logan Zoellner,4mo
1634,Deepmind's Gato: Generalist AgentΩ,Daniel Kokotajlo,4mo
1635,"""A Generalist Agent"": New DeepMind Publication",1a3orn,4mo
1636,What's keeping concerned capabilities gain researchers from leaving the field?Q,sovran,4mo
1637,Positive outcomes under an unaligned AGI takeover,Yitz,4mo
1638,Covid 5/12/22: Other Priorities,Zvi,4mo
1639,How would public media outlets need to be governed to cover all political views?Q,ChristianKl,4mo
1640,What are your recommendations for technical AI alignment podcasts?Q,Evan_Gaensbauer,4mo
1641, [Intro to brain-like-AGI safety] 14. Controlled AGIΩ,Steven Byrnes,4mo
1642,"ProjectLawful.com: Eliezer's latest story, past 1M words",Eliezer Yudkowsky,4mo
1643,An Inside View of AI Alignment,Ansh Radhakrishnan,4mo
1644,Fighting in various places for a really long time,KatjaGrace,4mo
1645,Stuff I might do if I had covid,KatjaGrace,4mo
1646,Crises Don't Need Your Software,GabrielExists,4mo
1647,The limits of AI safety via debateΩ,Marius Hobbhahn,4mo
1648,Examining Armstrong's category of generalized models,Morgan_Rogers,5mo
1649,Dath Ilani Rule of Law,David Udell,5mo
1650,AI safety should be made more accessible using non text-based media,Massimog,5mo
1651,LessWrong Now Has Dark Mode,jimrandomh,5mo
1652,Ceiling Fan Air Filter,jefftk,4mo
1653,Conditions for mathematical equivalence of Stochastic Gradient Descent and Natural SelectionΩ,Oliver Sourbut,5mo
1654,AI Alignment YouTube Playlists,"jacquesthibs, remember",5mo
1655,When is AI safety research harmful?,NathanBarnard,5mo
1656,A Bird's Eye View of the ML Field [Pragmatic AI Safety #2]Ω,"Dan Hendrycks, ThomasW",5mo
1657,Introduction to Pragmatic AI Safety [Pragmatic AI Safety #1]Ω,"Dan Hendrycks, ThomasW",5mo
1658,Thought experiment: Imagine you were assigned to help a random person in your community become as peaceful and joyful as the most peaceful and joyful person you'd ever met. What would you try?Q,nonzerosum,5mo
1659,Updating Utility FunctionsΩ,"JustinShovelain, Joar Skalse",5mo
1660,Transcripts of interviews with AI researchers,Vael Gates,5mo
1661,"A reason behind bad systems, and moral implications of seeing this reason",Edward Pascal,5mo
1662,An Alternative Interpretation of Physics,dadadarren,5mo
1663,Jobs: Help scale up LM alignment research at NYUΩ,Sam Bowman,5mo
1664,Microphone on Electric Mandolin,jefftk,5mo
1665,Willing to be your music mentor in exchange for video editing mentorshipQ,monkymind,5mo
1666,[Scribble] Bad Reasons Behind Different Systems and a Story with No Good Moral,Rana Dexsin,5mo
1667,What is the best day to celebrate Smallpox Eradication Day?Q,Orborde,5mo
1668,"Ion Implantation: Theory, Equipment, Process, Alternatives",ethanmorse,5mo
1669,Demonstrating MWI by interfering human simulations,Yair Halberstadt,5mo
1670,Notes from a conversation with Ing. Agr. Adriana Balzarini,Pablo Repetto,5mo
1671,Long-term Short-term Happiness,Matt Goldwater,5mo
1672,Elementary Infra-BayesianismΩ,Jan,5mo
1673,Video and Transcript of Presentation on Existential Risk from Power-Seeking AI,Joe Carlsmith,5mo
1674,Algorithmic formalization of FDT?Q,shminux,5mo
1675,Best open-source textbooks (goal: make them collaborative)?Q,shuffled-cantaloupe,5mo
1676,Long COVID risk: How to maintain an up to date risk assessment so we can go back to normal life?Q,"Sameerishere, Zvi",5mo
1677,Experience on Meloxicam,jefftk,5mo
1678,What does Functional Decision Theory say to do in imperfect Newcomb situations?Q,Daniel_Eth,5mo
1679,The glorious energy boost I've gotten by abstaining from coffee,Sameerishere,5mo
1680,Sealed predictions thread,Zach Stein-Perlman,5mo
1681,Minimum Viable Alignment,HunterJay,5mo
1682,[Linkpost] diffusion magnetizes manifolds (DALL-E 2 intuition building),Paul Bricman,5mo
1683,scipy.optimize.curve_fit Is Awesome,niplav,5mo
1684,"Quote request: ""if even the Sun requires proof""Q",Smaug123,5mo
1685,"But What's Your *New Alignment Insight,* out of a Future-Textbook Paragraph?",David Udell,5mo
1686,D&D.Sci Divination: Nine Black Doves,aphyer,5mo
1687,"Is there a convenient way to make ""sealed"" predictions?Q","Daniel Kokotajlo, Quintin Pope",5mo
1688,Hard evidence that mild COVID cases frequently reduce intelligence,Trevor1,5mo
1689,"Innovation, Stagnation, and Paratrooper Operations",Davis_Kingsley,5mo
1690,Upgrading Imagination: The Promise Of DALL-E 2 As A Tool For Thought,Tharin,5mo
1691,The case for becoming a black-box investigator of language modelsΩ,Buck,4mo
1692,Open Problems in Negative Side Effect MinimizationΩ,"Fabian Schimpf, Lukas Fluri",5mo
1693,Your Utility Function is Your Utility Function,David Udell,5mo
1694,Getting GPT-3 to predict Metaculus questions,MathiasKB,5mo
1695,Moral Illusions,Martin Sustrik,5mo
1696,Contra Dance Mask Policy,jefftk,5mo
1697,Apply to the second iteration of the ML for Alignment Bootcamp (MLAB 2) in Berkeley [Aug 15 - Fri Sept 2]Ω,Buck,5mo
1698,"Write posts business-like, not story-like",shminux,5mo
1699,Ethan Caballero on Private Scaling Progress,Michaël Trazzi,5mo
1700,Frankenstein: A Modern AGI,Sable,5mo
1701,"Starting too many projects, finishing none",rockthecasbah,5mo
1702,Repeal the Foreign Dredge Act of 1906,Zvi,5mo
1703,What We Owe the Past,Austin Chen,5mo
1704,An easy win for hard decisions,Alex Lawsen ,5mo
1705,Deriving Conditional Expected Utility from Pareto-Efficient Decisions,Thomas Kwa,5mo
1706,High-stakes alignment via adversarial training [Redwood Research report]Ω,"DMZ, LawrenceC, Nate Thomas",5mo
1707,Chording Bass,jefftk,5mo
1708,Covid 5/5/22: A Lack of Care,Zvi,5mo
1709,What is bias in alignment terms?Q,Jonas Kgomo,5mo
1710,How to balance between process and outcome?Q,Nathan Helm-Burger,5mo
1711,What is a Glowfic?,Wes F,5mo
1712,What are the best examples of catastrophic resource shortages?,jasoncrawford,5mo
1713,[Book review] No nonsense meditation,Valdes,5mo
1714,[Book review] The anxiety toolkit,Valdes,5mo
1715,[Book review] Getting things done,Valdes,5mo
1716,[Book review] Atomic habits,Valdes,5mo
1717,Improving productivity and wellbeing,Valdes,5mo
1718,Negotiating Up and Down the Simulation Hierarchy: Why We Might Survive the Unaligned Singularity,David Udell,5mo
1719,Steer the Sun?Q,Shay,5mo
1720,What Was Your Best / Most Successful DALL-E 2 Prompt?Q,Evidential,5mo
1721,Most problems don't differ dramatically in tractability (under certain assumptions),Thomas Kwa,5mo
1722,Introducing the ML Safety Scholars ProgramΩ,"Dan Hendrycks, ThomasW, Mantas Mazeika, Oliver Zhang, Sidney Hough, Kevin Liu",5mo
1723,Various Alignment Strategies (and how likely they are to work),Logan Zoellner,5mo
1724,Does the “ugh field” phenomenon sometimes occur strongly enough to affect immediate sensory processing?Q,Rana Dexsin,5mo
1725,Why humans don’t learn to not recognize danger?Q,Joachim Bartosik,5mo
1726,What would be the impact of cheap energy and storage?Q,Yair Halberstadt,5mo
1727,Monthly Shorts 4/2022,Celer,5mo
1728,Notes on Empathy,David Gross,5mo
1729,Is evolutionary influence the mesa objective that we're interested in?,David Johnston,5mo
1730,Looking for someone to run an online seminar on human learningQ,Isaac King,5mo
1731,Home Antigen Tests Aren’t Useful For Covid Screening,Elizabeth,5mo
1732,Open & Welcome Thread - May 2022 ,Ruby,5mo
1733,Information security considerations for AI and the long term future,"Jeffrey Ladish, lennart",5mo
1734,My thoughts on nanotechnology strategy research as an EA cause area,Ben_Snodin,5mo
1735,What Would It Cost to Build a World-Class Dredging Vessel in America?Q,"Zvi, AllAmericanBreakfast",5mo
1736,So has AI conquered Bridge ?,Ponder Stibbons,5mo
1737,Proposal for EA fund about x-risks,Dmitry Savishchev,5mo
1738,Squires,lsusr,5mo
1739,[Linkpost] A conceptual framework for consciousness,Gunnar_Zarncke,5mo
1740,What DALL-E 2 can and cannot do,Swimmer963,4mo
1741,Predicting for charity,Austin Chen,5mo
1742,Is it desirable for the first AGI to be conscious?Q,Raphaël S,5mo
1743,ELK shaving,Miss Aligned AI,5mo
1744,[Linkpost] Value extraction via language model abduction,Paul Bricman,5mo
1745,Maybe I 100% Know Something. But I Probably Won’t Be Able To 100% Explain It,Matt Goldwater,5mo
1746,Six Months of ROSE,Evenflair,5mo
1747,How confident are we that there are no Extremely Obvious Aliens?Q,Logan Zoellner,5mo
1748,How to be skeptical about meditation/Buddhism,Viliam,5mo
1749,My Approach to Non-Literal Communication,Isaac King,5mo
1750,Narrative Syncing,AnnaSalamon,5mo
1751,"Less Wrong Community Weekend 2022, open for application!",UnplannedCauliflower,5mo
1752,One master. One apprentice.,lsusr,5mo
1753,Shared Car: One Year In,jefftk,5mo
1754,What is the solution to the Alignment problem?,Algon,5mo
1755,Why hasn't deep learning generated significant economic value yet?Q,"Alex_Altair, gwern",5mo
1756,Nuclear Energy - Good but not the silver bullet we were hoping for,Marius Hobbhahn,5mo
1757,Quick Thoughts on A.I. Governance,NicholasKross,5mo
1758,Discussion on Thomas Philippon's paper on TFP growth being linear,Arjun Yadav,5mo
1759,We transhumanists want immortality... But is it really possible?,superads91,5mo
1760,Note-Taking without Hidden Messages,Hoagy,5mo
1761,How good is spending?Q,tryactions,5mo
1762,[Linkpost] New multi-modal Deepmind model fusing Chinchilla with images and videos,p.b.,5mo
1763,Salvage Epistemology,jimrandomh,5mo
1764,Learning the smooth priorΩ,"Geoffrey Irving, Rohin Shah, evhub",5mo
1765,Do FDT (or similar) recommend reparations?Q,David Scott Krueger (formerly: capybaralet),5mo
1766,Saying no to the Appleman,Johannes C. Mayer,5mo
1767,Prize for Alignment Research TasksΩ,"stuhlmueller, William_S",5mo
1768,Increasing Demandingness in EA,jefftk,5mo
1769,"What is a training ""step"" vs. ""episode"" in machine learning?Q",Evan R. Murphy,5mo
1770,Facts Matter,mrdlm,5mo
1771,Two Prosocial Rejection Norms,Emrik,5mo
1772,Dath Ilan vs. Sid Meier's Alpha Centauri: Pareto Improvements,David Udell,5mo
1773,A Parable Of Explainability,George3d6,5mo
1774,3-bit filters,iivonen,5mo
1775,Doom sooner,Flaglandbase,5mo
1776,How Might an Alignment Attractor Look like?,shminux,5mo
1777,Virtue signaling is sometimes the best or the only metric we have,Holly_Elmore,5mo
1778,The Gospel of Martin Luther,lsusr,5mo
1779,Slides: Potential Risks From Advanced AI,Aryeh Englander,5mo
1780,Is alignment possible?Q,Shay,5mo
1781,Keep your protos in one repo,RobertM,5mo
1782,"Covid 4/28/22: Take My Paxlovid, Please",Zvi,5mo
1783,Jaan Tallinn's 2021 Philanthropy Overview,jaan,5mo
1784,Letter to my Squire,lsusr,5mo
1785,The Speed + Simplicity Prior is probably anti-deceptiveΩ,Yonadav Shavit,5mo
1786,If you’re very optimistic about ELK then you should be optimistic about outer alignment,Sam Marks,5mo
1787,The Game of Masks,Slimepriestess,5mo
1788,Law-Following AI 3: Lawless AI Agents Undermine Stabilizing AgreementsΩ,Cullen_OKeefe,5mo
1789,Law-Following AI 2: Intent Alignment + Superintelligence → Lawless AI (By Default)Ω,Cullen_OKeefe,5mo
1790,Law-Following AI 1: Sequence Introduction and StructureΩ,Cullen_OKeefe,5mo
1791,[Intro to brain-like-AGI safety] 13. Symbol grounding & human social instinctsΩ,Steven Byrnes,5mo
1792,[Link] Evidence of Fabricated Data in a Vitamin C trial by Paul E Marik et al in CHEST,Kenny,5mo
1793,SERI ML Alignment Theory Scholars Program 2022Ω,"Ryan Kidd, Victor Warlop, Oliver Zhang",5mo
1794,EU Maximizing in a Gloomy World,David Udell,5mo
1795,AI Alternative Futures: Scenario Mapping Artificial Intelligence Risk - Request for Participation (*Closed*) ,Kakili,5mo
1796,The case for turning glowfic into Sequences,Thomas Kwa,5mo
1797,Why Copilot Accelerates TimelinesΩ,Michaël Trazzi,5mo
1798,Universals of Morality: Toward Human-Centric Communication Platforms,scafaria,5mo
1799,Continental Philosophy as Undergraduate Mathematics,Jan,5mo
1800,dalle2 comments,nostalgebraist,5mo
1801,Make a neural network in ~10 minutes,Arjun Yadav,5mo
1802,Framings of Deceptive AlignmentΩ,peterbarnett,5mo
1803,[$20K in Prizes] AI Safety Arguments CompetitionΩ,"Dan Hendrycks, Kevin Liu, Oliver Zhang, ThomasW, Sidney Hough",5mo
1804,Why pessimism sounds smart,jasoncrawford,5mo
1805,What is being improved in recursive self improvement?Q,Conor Sullivan,5mo
1806,21 on 21,Amir Bolous,5mo
1807,[Request for Distillation] Coherence of Distributed Decisions  With Different Inputs Implies ConditioningΩ,johnswentworth,5mo
1808,Quadratic voting with automatic collusion?Q,GuySrinivasan,5mo
1809,Intuitions about solving hard problemsΩ,Richard_Ngo,5mo
1810,Key questions about artificial sentience: an opinionated guide,Robbo,5mo
1811,On Tables and Happiness,Alexander,5mo
1812,Rationalist Inspired Coming-of-age RitualsQ,iceplant,5mo
1813,Ukraine Post #11: Longer Term Predictions,Zvi,5mo
1814, Why I'm Not a Utilitarian in Modern America,DanB,5mo
1815,Examining Evolution as an Upper Bound for AGI Timelines,meanderingmoose,5mo
1816,AI safety raising awareness resources bleg,iivonen,5mo
1817,I Think Therefore Someone Is Or Was?,Matt Goldwater,5mo
1818,Anyone Familiar with Ground News?Q,jmh,5mo
1819,Slack gives you space to notice/reflect on subtle things,Raemon,5mo
1820,Calling for Student Submissions: AI Safety Distillation Contest,Aris,5mo
1821,Rationality Dojo,lsusr,5mo
1822,Where can I publish an article containing a list of intellectuals who publicly admitted their mistakes once proven wrong?Q,Hashem ElAssad,5mo
1823,What Is a Major Chord?,jefftk,5mo
1824,DeletionQ,011eNigma235,5mo
1825,Re: So You Want to Be a Dharma Teacher,lsusr,5mo
1826,Ineffective Altruism,lsusr,5mo
1827,Has anyone written a reductionist theory of creativity?Q,Grant Demaree,5mo
1828,Progress Report 5: tying it together,Nathan Helm-Burger,5mo
1829,[ASoT] Consequentialist models as a superset of mesaoptimizersΩ,leogao,5mo
1830,Report likelihood ratios,Ege Erdil,5mo
1831,Skilling-up in ML Engineering for Alignment: request for comments,"TheMcDouglas, Jamie Bernardi",5mo
1832,Long Review: Economic Hierarchies,JohnBuridan,5mo
1833,PD-alikes in two dimensions,philh,5mo
1834,The Consistency Mystery,Flaglandbase,5mo
1835,Wanting to change what you wantQ,Mithrandir,5mo
1836,Replicating and extending the grabby aliens model,Tristan Cook,5mo
1837,The New Right appears to be on the rise for better or worse,Chris_Leong,5mo
1838,Mesa-utility functions might not be purely proxy goals,Thomas Kwa,5mo
1839,Why “progress studies” is interdisciplinary,jasoncrawford,5mo
1840,On Successful Communication Across a Wide Inferential Distance,Mahdi Complex,5mo
1841,Should I buy roofies from the darknet?,Malmesbury,5mo
1842,(When) do high-dimensional spaces have linear paths down to local minima?Q,Thomas Kwa,5mo
1843,All I Know is that I Know Nothing,lsusr,5mo
1844,Unity of Doctrine vs Unity of Method in Philosophy,JonathanErhardt,5mo
1845,Humanity as an entity: An alternative to Coherent Extrapolated Volition,ZT5,5mo
1846,AI Will Multiply,harsimony,5mo
1847,Infra-TopologyΩ,Diffractor,5mo
1848,Infra-MiscellaneaΩ,Diffractor,5mo
1849,Instrumental Convergence To Offer Hope?,michael_mjd,5mo
1850,China Covid #2,Zvi,5mo
1851,The Fetishization of Dystopia,john yardsale,5mo
1852,April was weird,lsusr,5mo
1853,Will/Has the Russia-Ukraine war been a tipping point for the shift from oil energy?Q,jmh,5mo
1854,Distributed blind review site for papers,Jonas Kgomo,5mo
1855,Preregistration: Air Conditioner Test,johnswentworth,5mo
1856,Are smart people's personal experiences biased against general intelligence?,tailcalled,5mo
1857,My Terrible Experience with Terror,monkymind,5mo
1858,THE GOLDEN RULE; What can we learn from it?,Rami Rustom,5mo
1859,Choice := Anthropics uncertainty? And potential implications for agencyQ,Antoine de Scorraille,5mo
1860,Reflections on My Own Missing Mood,Conor Sullivan,5mo
1861,Understanding the Merging of Opinions with Increasing Information theorem,ViktoriaMalyasova,5mo
1862,"For every choice of AGI difficulty, conditioning on gradual take-off implies shorter timelines.Ω",Francis Rhys Ward,5mo
1863,When is positive self-talk not worth it due to self-delusion?Q,"Jack R, romeostevensit",5mo
1864,I Caught Covid And All I Got Was This Lousy Ambiguous Data,Elizabeth,5mo
1865,Covid 4/21/22: Variants Working Overtime,Zvi,5mo
1866,When Can You Use a Travel Adapter?,jefftk,5mo
1867,Can GPT-N help us?Q,zfurman,5mo
1868,"Is growth linear, not exponential?",jasoncrawford,5mo
1869,How to place a bet on the end of the world,AllAmericanBreakfast,5mo
1870,"If everything is genetic, then nothing is genetic - Understanding the phenotypic null hypothesis",tailcalled,5mo
1871,Axis oriented programming,Donald Hobson,5mo
1872,[Intro to brain-like-AGI safety] 12. Two paths forward: “Controlled AGI” and “Social-instinct AGI”Ω,Steven Byrnes,5mo
1873,GPT-3 and concept extrapolationΩ,Stuart_Armstrong,5mo
1874,"A very quick analogy regarding ""opinions""",Magnus,5mo
1875,Why No *Interesting* Unaligned Singularity?,David Udell,5mo
1876,Judge Overturns Transportation Mask Mandate,Zvi,5mo
1877,Another argument that you will let the AI out of the box,Garrett Baker,5mo
1878,“Pivotal Act” Intentions: Negative Consequences and Fallacious ArgumentsΩ,Andrew_Critch,5mo
1879,Deceptive Agents are a Good Way to Do Things,David Udell,5mo
1880,The Scale Problem in AI,tailcalled,5mo
1881,Fixed points and free will,Ege Erdil,5mo
1882,The two missing core reasons why aligning at-least-partially superhuman AGI is hardQ,Joel Burget,5mo
1883,Three useful types of akrasia,Dambi,5mo
1884,"The internet makes it easier to cooperate, that’s the problem",David Hugh-Jones,5mo
1885,Concept extrapolation: key postsΩ,Stuart_Armstrong,5mo
1886,[Closed] Hiring a mathematician to work on the learning-theoretic AI alignment agenda,Vanessa Kosoy,5mo
1887,"What's the Relationship Between ""Human Values"" and the Brain's Reward System?Q","interstice, Quintin Pope",5mo
1888,Limitations of Laplace’s rule of succession,Tom Davidson,5mo
1889,Clarification on Definition of AGIQ,stanislaw,5mo
1890,How does the world look like 10 years after we have deployed an aligned AGI?Q,mukashi,5mo
1891,Fundamental Uncertainty: Chapter 2 - Why do words have meaning?,G Gordon Worley III,5mo
1892,Ensembling the greedy doctor problem,Ryan Kidd,5mo
1893,Mental Health and the Alignment Problem: A Compilation of Resources,Chris Scammell,5mo
1894,"Is ""Control"" of a Superintelligence Possible?",Mahdi Complex,5mo
1895,Are deference games a thing?,Daniel Kokotajlo,5mo
1896,Don't be afraid of the thousand-year-old vampire,Ulisse Mini,5mo
1897,Code Generation as an AI risk setting,Not Relevant,5mo
1898,Stupid Reasons to Have More Kids,JohnBuridan,5mo
1899,Org announcement: [AC]RC,Vivek Hebbar,5mo
1900,Fuck Your Miracle Year,rogersbacon,5mo
1901,What is causality to an evidential decision theorist?,paulfchristiano,5mo
1902,Learning as closing feedback loops,ambigram,5mo
1903,Alignment and Deep Learning,Aiyen,5mo
1904,Modect Englich Cpelling Reformc,G Gordon Worley III,5mo
1905,Do universities have an incentive to ignore data ethics as more small sample bias is brought to light? Q,hatta_afiq,5mo
1906,Fiction: My alternate earth story.,Donald Hobson,5mo
1907,Consciousness: A Compression-Based Approach,interstice,5mo
1908,Pop Culture Alignment Research and Taxes,Jan,5mo
1909,Introducing Mnosis,Evenflair,5mo
1910,What recent academic areas of study have a sample size that is 'too small'? Q,hatta_afiq,5mo
1911,Clem's Memo,abstractapplic,5mo
1912,Contra Alexander on the Virtue of Silence,benjaminalt,5mo
1913,Did OpenAI let GPT out of the box?Q,ChristianKl,5mo
1914,Constraining narrow AI in a corporate settingQ,MaximumLiberty,5mo
1915,"Summary: ""How to Read Books and Write Essays"" by OSP's Blue",Pablo Repetto,5mo
1916,Everything I Need To Know About Takeoff Speeds I Learned From Air Conditioner Ratings On AmazonΩ,johnswentworth,5mo
1917,Convince me that humanity *isn’t* doomed by AGIQ,"Yitz, mukashi",5mo
1918,Some reasons why a predictor wants to be a consequentialistΩ,Lauro Langosco,5mo
1919,Why Go is a Better Game than Chess,monkymind,5mo
1920,How to become an AI safety researcher,peterbarnett,5mo
1921,Announcing the Forecasting Wiki ,nikos,5mo
1922,How path-dependent are human values?Q,Ege Erdil,5mo
1923,Pivotal acts from Math AIs,azsantosk,5mo
1924,Feature proposal: Close comment as resolved,Viliam,5mo
1925,Feature proposal: Shortform reset,Viliam,5mo
1926,Refine: An Incubator for Conceptual Alignment Research BetsΩ,adamShimi,5mo
1927,My least favorite thing,sudo -i,5mo
1928,Early 2022 Paper Round-upΩ,jsteinhardt,5mo
1929,Only Asking Real Questions,jefftk,5mo
1930,What is the most significant way you have changed your mind in the last year?Q,"ChristianKl, Quintin Pope",5mo
1931,Lies Told To Children,Eliezer Yudkowsky,5mo
1932,Features that make a report especially helpful to me,lukeprog,5mo
1933,Exploring toy neural nets under node removal. Section 1.,Donald Hobson,5mo
1934,Can someone explain to me why MIRI is so pessimistic of our chances of survival?Q,iamthouthouarti,5mo
1935,Redwood Research is hiring for several roles (Operations and Technical),"Jessica W, billzito",5mo
1936,Covid 4/14/22: China Stays the Course,Zvi,5mo
1937,Unchangeable Code possible ?Q,AntonTimmer,5mo
1938,Emacs Recentering With Context,jefftk,5mo
1939,Make a Movie Showing Alignment Failures,Logan Riggs,5mo
1940,"Summary: ""How to Do Research"" by OSP's Red",Pablo Repetto,5mo
1941,A Quick Guide to Confronting Doom,Ruby,5mo
1942,"Design, Implement and Verify",rwallace,5mo
1943,Takeoff speeds have a huge effect on what it means to work on AI x-riskΩ,Buck,5mo
1944,What to include in a guest lecture on existential risks from AI?QΩ,Aryeh Englander,5mo
1945,Common Knowledge is a Circle Game for Toddlers,ryan_b,5mo
1946,Another list of theories of impact for interpretabilityΩ,Beth Barnes,5mo
1947,The Cage of the Language,Martin Sustrik,5mo
1948,How dath ilan coordinates around solving alignment,Thomas Kwa,5mo
1949,What more compute does for brain-like models: response to Rohin,Nathan Helm-Burger,5mo
1950,“Fragility of Value” vs. LLMsQ,Not Relevant,5mo
1951,"Commensurable Scientific Paradigms; or, computable induction",samshap,5mo
1952,Convincing People of Alignment with Street Epistemology,Logan Riggs,5mo
1953,"What's a good probability distribution family (e.g. ""log-normal"") to use for AGI timelines?QΩ",David Scott Krueger (formerly: capybaralet),5mo
1954,The Peerless,carado,5mo
1955,Useful Vices for Wicked Problems,HoldenKarnofsky,5mo
1956,Genetic Enhancement: a Strategy for Long(ish) AGI Timeline Worlds,kman,5mo
1957,A Small Negative Result on DebateΩ,Sam Bowman,5mo
1958,Is technical AI alignment research a net positive?,cranberry_bear,5mo
1959,Reward model hacking as a challenge for reward learning,Erik Jenner,5mo
1960,How I use Anki: expanding the scope of SRS,TheMcDouglas,5mo
1961,Favorites & Performers,Soma,5mo
1962,A broad basin of attraction around human values?Ω,Wei_Dai,5mo
1963,The Platonist’s Dilemma: A Remix on the Prisoner's.,James Camacho,5mo
1964,Does the rationalist community have a membership funnel?Q,Alex_Altair,5mo
1965,US Taxes: Adjust Withholding When Donating?,jefftk,5mo
1966,Introducing Effective Self-Help,Ben Williamson,5mo
1967,Ukraine Post #10: Next Phase,Zvi,5mo
1968,"What is your advice for elder care, particularly taking care of dementia patients?Q","JohannWolfgang, cranberry_bear",5mo
1969,What do you think will most probably happen to our consciousness when our simulation ends?Q,ArtMi,5mo
1970,"AI governance student hackathon on Saturday, April 23: register now!",Michael Chen,5mo
1971,Rambling thoughts on having multiple selves,cranberry_bear,5mo
1972,An AI-in-a-box success model,azsantosk,5mo
1973,The Regulatory Option: A response to near 0% survival odds,Matthew Lowenstein,5mo
1974,The Efficient LessWrong Hypothesis - Stock Investing Competition,ViktorThink,5mo
1975,Review: Structure and Interpretation of Computer Programs,LRudL,5mo
1976,The Glitch And Notes On Digital Beings,Ghvst,5mo
1977,Underappreciated content on LessWrongQ,Ege Erdil,5mo
1978,Editing Advice for LessWrong Users,JustisMills,5mo
1979,Post-history is written by the martyrs,Veedrac,5mo
1980,What can people not smart/technical enough for AI research/AI risk work do to reduce AI-risk/maximize AI safety? (which is most people?),Alex K. Chen,5mo
1981,Goodhart's Law Causal Diagrams,"JustinShovelain, Jeremy Gillen",5mo
1982,Is it time to start thinking about what AI Friendliness means?,ZT5,5mo
1983,Is there an equivalent of the CDF for grading predictions?Q,Optimization Process,5mo
1984,We should stop being so confident that AI coordination is unlikely,Trevor1,5mo
1985,What Chords Do You Need?,jefftk,5mo
1986,China Covid Update #1,Zvi,5mo
1987,"Convince me that humanity is as doomed by AGI as Yudkowsky et al., seems to believeQ","Yitz, Eliezer Yudkowsky",5mo
1988,Emotionally Confronting a Probably-Doomed World: Against Motivation Via Dignity Points,TurnTrout,5mo
1989,Does non-access to outputs prevent recursive self-improvement?Q,Gunnar_Zarncke,5mo
1990,Finally Entering Alignment,Ulisse Mini,5mo
1991,Is Fisherian Runaway Gradient Hacking?,Ryan Kidd,5mo
1992,Worse than an unaligned AGI,shminux,6mo
1993,Time-Time Tradeoffs,Akash,6mo
1994,A Brief Excursion Into Molecular Neuroscience,Jan,5mo
1995,Boston Contra: Fully Gender-Free,jefftk,6mo
1996,Hidden comments settings not working?Q,TLW,6mo
1997,Godshatter Versus Legibility: A Fundamentally Different Approach To AI Alignment,LukeOnline,6mo
1998,A concrete bet offer to those with short AI timelines,"Matthew Barnett, Tamay",6mo
1999,New: use The Nonlinear Library to listen to the top LessWrong posts of all time,KatWoods,6mo
2000,140 Cognitive Biases You Should Know,André Ferretti,6mo
2001,"
Strategies for keeping AIs narrow in the short term",Rossin,6mo
2002,Hyperbolic takeoff,Ege Erdil,6mo
2003,Elicit: Language Models as Research AssistantsΩ,"stuhlmueller, jungofthewon",6mo
2004,"AMA Conjecture, A New Alignment StartupΩ",adamShimi,6mo
2005,What advice do you have for someone struggling to detach their grim-o-meter?Q,Zorger74,6mo
2006,Can AI systems have extremely impressive outputs and also not need to be aligned because they aren't general enough or something?Q,WilliamKiely,6mo
2007,Buy-in Before Randomization,jefftk,6mo
2008,Why Instrumental Goals are not a big AI Safety Problem,Jonathan Paulson,6mo
2009,Emergent Ventures/Schmidt (new grantor for individual researchers),gwern,6mo
2010,A method of writing content easily with little anxiety,jessicata,6mo
2011,Roam Research Mobile is Out!,Logan Riggs,6mo
2012,Progress Report 4: logit lens redux,Nathan Helm-Burger,6mo
2013,What would the creation of aligned AGI look like for us?Q,Perhaps,6mo
2014,Convincing All Capability Researchers,Logan Riggs,6mo
2015,Language Model Tools for Alignment ResearchΩ,Logan Riggs,6mo
2016,Takeaways From 3 Years Working In Machine Learning,George3d6,6mo
2017,"AIs should learn human preferences, not biasesΩ",Stuart_Armstrong,6mo
2018,Different perspectives on concept extrapolationΩ,Stuart_Armstrong,6mo
2019,Is there a possibility that the upcoming scaling of data in language models causes A.G.I.?Q,ArtMi,6mo
2020,Good Heart Week Is Over!,Ben Pace,6mo
2021,Good Heart Donation Lottery Winner,G Gordon Worley III,6mo
2022,If Dumbledore Was Named Eldore,TourmalineCupcakes,6mo
2023,[RETRACTED] It's time for EA leadership to pull the short-timelines fire alarm.,Not Relevant,6mo
2024,"We Are Conjecture, A New Alignment Research StartupΩ",Connor Leahy,6mo
2025,The Rationalist-Etcetera Diaspora: A SPREADSHEET!!,Amelia Bedelia,6mo
2026,DeepMind: The Podcast - Excerpts on AGI,WilliamKiely,6mo
2027,Convincing Your Brain That Humanity is Evil is Easy,Johannes C. Mayer,6mo
2028,The Explanatory Gap of AI,David Valdman,6mo
2029,Believable near-term AI disaster,Dagon,6mo
2030,List of concrete hypotheticals for AI takeover?Q,Yitz,6mo
2031,"What if ""friendly/unfriendly"" GAI isn't a thing?",homunq,6mo
2032,"Productive Mistakes, Not Perfect AnswersΩ",adamShimi,6mo
2033,Duncan Sabien On Writing,lynettebye,6mo
2034,[ASoT] Some thoughts about imperfect world modelingΩ,leogao,6mo
2035,How  BoMAI Might failΩ,Donald Hobson,6mo
2036,Is GPT3 a Good Rationalist? - InstructGPT3 [2/2],WayZ,6mo
2037,I discovered LessWrong... during Good Heart Week,identity.key,6mo
2038,Research agenda - Building a multi-modal chess-language model,p.b.,6mo
2039,Playing with DALL·E 2,Dave Orr,6mo
2040,Covid 4/7/22: Opening Day,Zvi,6mo
2041,Predicting a global catastrophe: the Ukrainian model,RomanS,6mo
2042,Control the Density of Novelty in Your Writing,En Kepeig,6mo
2043,Elasticity of Wheat Supply?Q,johnswentworth,6mo
2044,The Debtors' Revolt,Benquo,6mo
2045,How to Interpret Vitamin D Dosage Using Numbers,Benquo,6mo
2046,Notes on the Autobiography of Malcolm X,Benquo,6mo
2047,Understanding Gödel's Incompleteness Theorem,Rafael Harth,6mo
2048,The Zombie Argument for Empiricists,JonathanErhardt,6mo
2049,What I Was Thinking About Before Alignment,johnswentworth,6mo
2050,[Link] A minimal viable product for alignmentΩ,janleike,6mo
2051,[Link] Why I’m excited about AI-assisted human feedbackΩ,janleike,6mo
2052,Strategic Considerations Regarding Autistic/Literal AI,Chris_Leong,6mo
2053,DALL·E 2 by OpenAI,P.,6mo
2054,[Intro to brain-like-AGI safety] 11. Safety ≠ alignment (but they’re close!)Ω,Steven Byrnes,6mo
2055,Research agenda: Can transformers do system 2 thinking?,p.b.,6mo
2056,"PaLM in ""Extrapolating GPT-N performance""Ω",Lanrian,6mo
2057,"What Twitter fixes should we advocate, now that Elon is on the board?Q",Jackson Wagner,6mo
2058,Prioritise Tasks by Rating not Sorting,Neel Nanda,6mo
2059,"Supervise Process, not OutcomesΩ","stuhlmueller, jungofthewon",6mo
2060,My agenda for research into transformer capabilities - Introduction,p.b.,6mo
2061,Forecasting Newsletter: March 2022,NunoSempere,6mo
2062,What Would A Fight Between Humanity And AGI Look Like?,johnswentworth,6mo
2063,Prompt Your Brain,En Kepeig,6mo
2064,Save Humanity! Breed Sapient Octopuses!,Yair Halberstadt,6mo
2065,"Ideal governance (for companies, countries and more)",HoldenKarnofsky,5mo
2066,Non-programmers intro to AI for programmers,Dustin,6mo
2067,The case for Doing Something Else (if Alignment is doomed),Rafael Harth,6mo
2068,My Transhuman Dream,Johannes C. Mayer,6mo
2069,Are the fundamental physical constants computable?,Yair Halberstadt,6mo
2070,5-Minute Advice for EA Global,Logan Riggs,6mo
2071,Explaining the Twitter Postrat Scene,Jacob Falkovich,6mo
2072,Ukraine Post #9: Again,Zvi,6mo
2073,Optimizing crop planting with mixed integer linear programming in Stardew Valley,hapanin,6mo
2074,Software Engineering: Getting Hired and Promoted,G Gordon Worley III,6mo
2075,Over-Organization (Chapter 5),Justin Bullock,6mo
2076,Greyed Out Options,ozymandias,6mo
2077,App Review: Amaru,ozymandias,6mo
2078,"Debt, Submission, and a Technical Definition of Drama",Benquo,6mo
2079,Inner Ring as Adversary,Benquo,6mo
2080,"Language, Power, and the Categorical Imperative",Benquo,6mo
2081,Civil Law and Political Drama,Benquo,6mo
2082,Is the scaling race finally on?,p.b.,6mo
2083,Premortem as communication device (e.g. in relationship),becausecurious,6mo
2084,Call For DistillersΩ,johnswentworth,5mo
2085,Google's new 540 billion parameter language model ,Matthew Barnett,6mo
2086,Best informative videos on the InternetQ,Ege Erdil,6mo
2087,Not-Useless Advice For Dealing With Things You Don't Want to Do,Rafael Harth,6mo
2088,Dr Fauci as Machiavellian Boddhisattva,Benquo,6mo
2089,Ways to do advanced tag filteringQ,lizard_brain,6mo
2090,Giving calibrated time estimates can have social costs,Alex_Altair,6mo
2091,The Case for Frequentism: Why Bayesian Probability is Fundamentally Unsound and What Science Does Instead,lsusr,6mo
2092,The fingerprints of ideology in science,Malmesbury,6mo
2093,How to write a LW sequence to learn a topic?Q,PabloAMC,6mo
2094,Should we push for banning making hiring decisions based on AI?Q,ChristianKl,6mo
2095,Working Out in VR Really Works,Yonatan Cale,6mo
2096,On Agent Incentives to Manipulate Human Feedback in Multi-Agent Reward Learning ScenariosΩ,Francis Rhys Ward,6mo
2097,"Science is Mining, not Foraging",Seb Farquhar,6mo
2098,A simple guide to life,jasoncrawford,6mo
2099,How Real Moral Mazes (in Bay Area startups)?,G Gordon Worley III,6mo
2100,[Book Review] Why Greatness Cannot Be Planned: The Myth of the Objective,Stuckwork,6mo
2101,"""Don't Get Mad, Get Curious""",Rubix,6mo
2102,The doomsday argument is normal,avturchin,6mo
2103,[Invisible Networks] Goblin Marketplace,Kaj_Sotala,6mo
2104,Avoiding Moral Fads?,Davis_Kingsley,6mo
2105,Very Dark Mouth-Shaped Non-fancy Chocolate?,jefftk,6mo
2106,20 Modern Heresies,rogersbacon,6mo
2107,Approach to Screen Time,jefftk,6mo
2108,Uncontrollable Super-Powerful Explosives,Sammy Martin,6mo
2109,Variadic functions in Hindley Milner,philh,6mo
2110,Book review: Very Important People,Richard_Ngo,6mo
2111,Real biosafety is socially inconvenient,ChristianKl,6mo
2112,How I repeatedly failed to use Tobit modelling on censored data,abstractapplic,6mo
2113,Noobs Need Rules,Jacob Falkovich,6mo
2114,What words should be shorter in the rational dictionary?Q,brook,6mo
2115,[Invisible Networks] Psyche-Sort,Kaj_Sotala,6mo
2116,Moloch and the sandpile catastrophe,Eric Raymond,6mo
2117,Moral Anti-Realism: Introduction & Summary,Lukas_Gloor,6mo
2118,Retrospective: Practical Social Networking,Evenflair,6mo
2119,Optional stopping,Ege Erdil,6mo
2120,How Money Fails to Track Value,JustinShovelain,6mo
2121,Unfinished Projects Thread,Gunnar_Zarncke,6mo
2122,Interacting with a Boxed AI,aphyer,6mo
2123,Two Forms of Moral Judgment,aphyer,6mo
2124,Manafold Markets is out of mana 🤭,Austin Chen,6mo
2125,The median and mode use less information than the mean does,Maxwell Peterson,6mo
2126,New Product MVP: LightWrong,E. Garrett,6mo
2127,Towards trying to feel consistently energized,Maxwell Peterson,6mo
2128,New Scaling Laws for Large Language ModelsΩ,1a3orn,6mo
2129,[Link] sona ike lili,AprilSR,6mo
2130,"Questions about ''formalizing instrumental goals""",Mark Neyer,6mo
2131,The real reason Futarchists are doomed,lc,6mo
2132,"To Make Better Software, Do What Artists Do",matto,6mo
2133,Announcing Impact Island: A New EA Reality TV Show,Linch,6mo
2134,"If you lose enough Good Heart Tokens, will you lose real-world money?Q",Yitz,6mo
2135,What's the problem with Oracular AIs?Q,FinalFormal2,6mo
2136,5 Tips for Good Hearting,Thomas Kwa,6mo
2137,Good Heart Donation LotteryQ,G Gordon Worley III,6mo
2138,Intro to hacking with the lambda calculus,LRudL,6mo
2139,The lure of technocracy,jasoncrawford,6mo
2140,The Halting Problem and the Impossible Photocopier,Jemist,6mo
2141,[Link] Training Compute-Optimal Large Language ModelsΩ,nostalgebraist,6mo
2142,Newcomb’s problem is just a standard time consistency problem,basil.halperin,6mo
2143,AXRP Episode 13 - First Principles of AGI Safety with Richard NgoΩ,DanielFilan,6mo
2144,Confidence Levels in Forecasts and Psychological SurveysQ,rodeo_flagellum,6mo
2145,Covid 3/31/2022: More of the Same,Zvi,6mo
2146,They Don’t Know About Second Booster,Zvi,6mo
2147,Ways to organize a search for a missing person in LA from abroad?Q,calmarrr,6mo
2148,A bicycle for your memory,sortega,6mo
2149,ELK Computational Complexity: Three Levels of DifficultyΩ,abramdemski,6mo
2150,Abandoned prototype video game teaching elementary circuit theory,Steven Byrnes,6mo
2151,"No, EDT Did Not Get It Right All Along: Why the Coin Flip Creation Problem Is Irrelevant",Heighn,6mo
2152,How to Make Your Article Change People's Minds or Actions? (Spoiler: Do User Testing Like a Startup Would),Yonatan Cale,6mo
2153,Procedurally evaluating factual accuracy: a request for researchΩ,Jacob_Hilton,6mo
2154,[ASoT] Some thoughts about LM monologue limitations and ELKΩ,leogao,6mo
2155,[Intro to brain-like-AGI safety] 10. The alignment problemΩ,Steven Byrnes,6mo
2156,Progress Report 2,Nathan Helm-Burger,6mo
2157,What would make you confident that AGI has been achieved?Q,Yitz,6mo
2158,Meta wants to use AI to write Wikipedia articles; I am Nervous™,Yitz,6mo
2159,Beyond Blame Minimization: Thoughts from the comments,physicaleconomics,6mo
2160,Gears-Level Mental Models of Transformer InterpretabilityΩ,KevinRoWang,6mo
2161,Debating myself on whether “extra lives lived” are as good as “deaths prevented”,HoldenKarnofsky,6mo
2162,Hinges and crises,"Jan_Kulveit, technicalities",6mo
2163,Strategies for differential divulgation of key ideas in AI capability,azsantosk,6mo
2164,Towards a better circuit prior: Improving on ELK state-of-the-artΩ,evhub,6mo
2165,"Summary: ""How to read a book"" by Mortimer Adler",Pablo Repetto,6mo
2166,Retreat for rationality meetup organizers (apply to attend!),mingyuan,6mo
2167,Can we simulate human evolution to create a somewhat aligned AGI? ,Thomas Kwa,6mo
2168,Mental nonsense: my anti-insomnia trick,AllAmericanBreakfast,6mo
2169,Vaniver's ELK SubmissionΩ,Vaniver,6mo
2170,[ASoT] Some thoughts about deceptive mesaoptimizationΩ,leogao,6mo
2171,Humans pretending to be robots pretending to be human,Richard_Kennaway,6mo
2172,Consequentialist veganism Q,tslarm,6mo
2173,Toward a general structure of economic explanation,physicaleconomics,6mo
2174,Do a cost-benefit analysis of your technology usage,TurnTrout,6mo
2175,Ukraine Post #7: Prediction Market Update,Zvi,6mo
2176,How many generals does Russia have left?Q,Yitz,6mo
2177,Sums and products,"Ege Erdil, Metaculus",6mo
2178,[ASoT] Searching for consequentialist structureΩ,leogao,6mo
2179,Practical everyday human strategizing,akaTrickster,6mo
2180,Manhattan project for aligned AI,Chris van Merwijk,6mo
2181,[ASoT] Some ways ELK could still be solvable in practiceΩ,leogao,6mo
2182,Beyond Blame Minimization,physicaleconomics,5mo
2183,Your specific attitudes towards AI safetyQ,Esben Kran,6mo
2184,"What are the top 1-10 posts / sequences / articles / etc. that you've found most useful for yourself for becoming ""less wrong""?Q",Aryeh Englander,6mo
2185,"When people ask for your P(doom), do you give them your inside view or your betting odds?QΩ",Vivek Hebbar,6mo
2186,"A Primer on God, Liberalism and the End of History",Mahdi Complex,6mo
2187,Agency and Coherence,David Udell,6mo
2188,"Magic, tricks, and high-dimensional configuration spaces",pchvykov,6mo
2189,Compute Governance: The Role of Commodity HardwareΩ,Jan,6mo
2190,Game that might improve research productivity,Jack R,6mo
2191,You must on average be improving to not guarantee failure.,eeegnu,6mo
2192,Primitive Perspectives and Sleeping Beauty,dadadarren,6mo
2193,[ASoT] Observations about ELKΩ,leogao,6mo
2194,Lesson Plan: Biases in Quantity Estimation,abstractapplic,6mo
2195,Your risk of developing long COVID is probably high,Siebe,6mo
2196,Why Agent Foundations? An Overly Abstract ExplanationΩ,johnswentworth,6mo
2197,"Summary: ""How to write a thesis"" by Umberto Eco",Pablo Repetto,6mo
2198,Is Metaculus Slow to Update?,SimonM,6mo
2199,Duels & D.Sci March 2022: It's time for D-d-d-d-d-d-d-d-d-d-d-d-d-d-data!,aphyer,6mo
2200,Contextual Self,ACrackedPot,6mo
2201,Fake Journal Club proposal,gwern,6mo
2202,My mistake about the war in Ukraine,Ege Erdil,6mo
2203,An outline of an ironic LessWrong post,Yitz,6mo
2204,Ukraine Post #6: More Data and Peace Terms,Zvi,6mo
2205,Chesterton’s Fence vs The Onion in the Varnish,Joel Burget,6mo
2206,The Ontics and The Decouplers,hawkebia,6mo
2207,What if fiat money was created at a fixed rate and given to government?Q,ViktorThink,6mo
2208,Christopher Alexander's architecture for learning,Henrik Karlsson,6mo
2209,Experimental longtermism: theory needs data,"Jan_Kulveit, technicalities",6mo
2210,"On expected utility, part 4: Dutch books, Cox, and Complete Class",Joe Carlsmith,6mo
2211,In the very near future the internet will answer all of our questions and that makes me sad,David Gross,6mo
2212,Covid 3/24/22: Respite,Zvi,6mo
2213,A survey of tool use and workflows in alignment researchΩ,"Logan Riggs, Jan, janus, jacquesthibs",6mo
2214,Rational and irrational infinite integers,Viliam,6mo
2215,Research on how pattern-finding contributes to memorization?Q,AllAmericanBreakfast,6mo
2216,Why Miracles Should Not Be Used as a Reason to Believe in a Religion,Yitz,6mo
2217,Announcing research,Pablo Repetto,6mo
2218,Flywheels of progress,jasoncrawford,6mo
2219,"Circular Counterfactuals ""Only that which Happens is Possible""",JohnBuridan,6mo
2220,[Intro to brain-like-AGI safety] 9. Takeaways from neuro 2/2: On AGI motivationΩ,Steven Byrnes,6mo
2221,Luna Lovegood and the Fidelius Curse - Part 12,lsusr,6mo
2222,"Different Is The Generator, Not A Side Effect",George3d6,6mo
2223,Job Offering: Help Communicate Infrabayesianism,"abramdemski, Vanessa Kosoy, Diffractor",6mo
2224,Wanted: Executive Assistant to help build the progress movement,jasoncrawford,6mo
2225,Cyberwar escalation,ChristianKl,6mo
2226,Albert Hirschman on liberalism and nationalism,David Hugh-Jones,6mo
2227,Lessons After a Couple Months of Trying to Do ML Research,KevinRoWang,6mo
2228,"Progress Report 1: interpretability experiments & learning, testing compression hypotheses",Nathan Helm-Burger,6mo
2229,Brain preservation to prevent involuntary death: a possible cause area,Andy_McKenzie,6mo
2230,Searching for post on Community TakeoverQ,Søren Elverlin,6mo
2231,"On expected utility, part 3: VNM, separability, and more",Joe Carlsmith,6mo
2232,Research Hamming Questions,johnswentworth,6mo
2233,Responsible AI communication survey,Anca G. Rusu,6mo
2234,Even more curated conversations with brilliant rationalists,spencerg,6mo
2235,Counter-theses on Sleep,Natália Coelho Mendonça,6mo
2236,"Jetlag, Nausea, and Diarrhea are Largely Optional",Thomas Kwa,6mo
2237,We cannot directly choose an AGI's utility function,azsantosk,6mo
2238,Why will an AGI be rational?,azsantosk,6mo
2239,The Dawn of Everything: a review,David Hugh-Jones,6mo
2240,Searching for outliers,benkuhn,6mo
2241,Civilization as Self-Restraint,Duncan_Sabien,6mo
2242,Ukraine Post #5: Bits of Information,Zvi,6mo
2243,How to ventilate,jefftk,6mo
2244,What are the best elementary math problems you know?Q,Ege Erdil,6mo
2245,Natural Value Learning,Chris van Merwijk,6mo
2246,How I fixed exercise,GabrielExists,6mo
2247,"Know of any randomised trials teaching debiasing in schools? Want to help review them, or would that review help your work?",MNoetel,6mo
2248,Trusting groups over individualsQ,castor07,6mo
2249,How to prevent authoritarian revolts?Q,lc,6mo
2250,On Context And People,Jan,6mo
2251,Exploring Finite Factored Sets with some toy examplesΩ,Thomas Kehrenberg,6mo
2252,Rationalists Should Learn Lock Picking,brook,6mo
2253,Wargaming AGI Development,ryan_b,6mo
2254,Can you be Not Even Wrong in AI Alignment?Q,throwaway8238,6mo
2255,"Goal-directedness: imperfect reasoning, limited knowledge and inaccurate beliefs",Morgan_Rogers,6mo
2256,Thoughts on the SPIES Forecasting Method?Q,rodeo_flagellum,6mo
2257,Retrospective: Context and Communication,The Guild of the Rose,6mo
2258,Notes on Social Responsibility,David Gross,6mo
2259,[Linkpost] 7 Swedish Words to Import,mike_hawke,6mo
2260,When will kids stop wearing masks at school?Q,"Daniel Kokotajlo, hath",6mo
2261,What to do after inventing AGI?Q,elephantcrew,6mo
2262,Cold Takes reader survey - let me know what you want more and less of!,HoldenKarnofsky,6mo
2263,Learned Blankness and Expectations of Solubility,brook,6mo
2264,"On expected utility, part 2: Why it can be OK to predictably lose",Joe Carlsmith,6mo
2265,Our time in history as evidence for simulation theory?Q,Garrett Garzonie,6mo
2266,On Defecting On Yourself,David Udell,6mo
2267,What Should I Do Today?Q,Cui,6mo
2268,Only needs a flying saucer,Mary Chernyshenko,6mo
2269,Nuclear Deterrence 101 (and why the US can't even hint at intervening in Ukraine),Darmani,6mo
2270,Phase transitions and AGI,"Ege Erdil, Metaculus",6mo
2271,Is Grabby Aliens built on good anthropic reasoning?Q,"Steven Byrnes, Tristan Cook",6mo
2272,Is there a link between Immigration Policy and Inflation?,hawkebia,6mo
2273,Using GPT-3 for preventing conflict during messaging — a pitch for an app,Eli_,6mo
2274,"What is the equivalent of the ""do"" operator for finite factored sets?Q",Chris van Merwijk,6mo
2275,"The One Who Says, ""Enough""",Sable,6mo
2276,Luna Lovegood and the Fidelius Curse - Part 11,lsusr,6mo
2277,Covid 3/17/22: The Rise of BA.2,Zvi,6mo
2278,"Why a No-Fly-Zone would be the biggest gift to Putin, and why Zelenskyy keeps asking for it [Linkpost and commentary]",Dojan,6mo
2279,Formal epistemiology for extracting truth from news sourcesQ,Enigma,6mo
2280,"On expected utility, part 1: Skyscrapers and madmen",Joe Carlsmith,6mo
2281,What do paradigm shifts look like?,leogao,6mo
2282,Thoughts on Crowdfunding with Refund Bonuses,Yoav Ravid,6mo
2283,Can you only realize objects in your mind after they appear?Q,Cui,6mo
2284,[Intro to brain-like-AGI safety] 8. Takeaways from neuro 1/2: On AGI developmentΩ,Steven Byrnes,6mo
2285,Replacing Natural Interpretations,adamShimi,6mo
2286,Some (potentially) fundable AI Safety Ideas,Logan Riggs,6mo
2287,[Quote] Why does i show up in Quantum Mechanics and other Beautiful Math Mysteries,Gunnar_Zarncke,6mo
2288,War Mode vs. Peace Mode,konstell,6mo
2289,Danger(s) of theorem-proving AI?Q,Yitz,6mo
2290,Do small studies add up?,one_forward,6mo
2291,I read Einstein's biography. Here are 15 quotes that reveal his philosophy on life.,dmabery,6mo
2292,My current thoughts on the risks from SETI,Matthew Barnett,6mo
2293,Smoking cigarettes in airplanes made the airplanes safer?,CraigMichael,6mo
2294,Probabilistic Negotiation,Robert Kennedy,6mo
2295,Dual use of artificial-intelligence-powered drug discoveryΩ,Vaniver,6mo
2296,Ukraine #4: Prediction Market Movement Modeling,Zvi,6mo
2297,AI researchers from Russia looking for a new home,AnonResearch,6mo
2298,Who is doing Cryonics-relevant research?Q,Wuschel Schulz,6mo
2299,ELK contest submission: route understanding through the human ontologyΩ,"Vika, Ramana Kumar, Vikrant Varma",6mo
2300,One possible approach to develop the best possible general learning algorithm,martillopart,6mo
2301,The Meaninglessness of it all,foxacc,6mo
2302,Technological Resurrection: Two Possible Approaches,Alex Polischuk,6mo
2303,[Linkpost] Growth in FLOPS used to train ML models,Derek M. Jones,6mo
2304,"Shotgun Book Reviews: Against Method, The Knowledge Machine and Understanding Nature",adamShimi,6mo
2305,Alex Tabarrok advocates for crowdfunding systems with *Refund Bonuses*. I think this might be a natural occurrence of a money pump against Causal Decision Theory pledgers,MakoYass,6mo
2306,May I Speak?,Raythen,6mo
2307,MicroCOVID risk levels inaccurate due to undercounting in data sources vs wastewater data? Q,Sameerishere,6mo
2308,Ethicality Behind Breaking the Glass Ceiling,foxacc,6mo
2309,Omicron #16: Danger in China,Zvi,6mo
2310,"Ukraine #3: Decision Theory, Madman Theory and the Mafioso Nature",Zvi,6mo
2311,Whence the determinant?,Ege Erdil,6mo
2312,Food manufacturers are out to get you,jwray,6mo
2313,How to Lumenate (UK Edition),chanamessinger,6mo
2314,New GPT3 Impressive Capabilities - InstructGPT3 [1/2],WayZ,6mo
2315,Anecdotes Can Be Strong Evidence and Bayes Theorem Proves It,FCCC,6mo
2316,Challenges to Yudkowsky's Pronoun Reform Proposal,Zack_M_Davis,6mo
2317,War-prompted Disaster Planning,jefftk,6mo
2318,"Game theory, sanctions, and Ukraine",Dumbledore's Army,6mo
2319,Compute Trends — Comparison to OpenAI’s AI and Compute,"lennart, Jsevillamol, Pablo Villalobos, Marius Hobbhahn, Tamay Besiroglu, anson.ho",6mo
2320,Is there a good dataset for the moments of the income distribution throughout history?Q,Ege Erdil,6mo
2321,Why Rome?,Erich_Grunewald,6mo
2322,"Book review of ""Mengzi""",Anonymous,6mo
2323,"If someone with sufficient capabilities decides to deliberately create an Unaligned AGI, is there anything anyone can actually DO, to stop them?Q",[anonymous],6mo
2324,Experience Restarting Contra,jefftk,6mo
2325,Bear Surprise Freedom Network,CraigMichael,6mo
2326,Your Future Self's Credences Should Be Unpredictable to You,David Udell,6mo
2327,Arguments are good for helping people reason about things,AprilSR,6mo
2328,Is There a Valley of Bad Civilizational Adequacy?,lbThingrb,6mo
2329,Deep Dives: My Advice for Pursuing Work in Research ,scasper,6mo
2330,"If your solution doesn't work, make it work",Ege Erdil,6mo
2331,Ways to invest your wealth if you believe in a high-variance future?Q,Alex_Altair,6mo
2332,A Longlist of Theories of Impact for InterpretabilityΩ,Neel Nanda,6mo
2333,Beyond micromarriages,Richard_Ngo,6mo
2334,On continuous decision boundaries,eeegnu,6mo
2335,Are there any impossibility theorems for strong and safe AI?Q,David Johnston,6mo
2336,Rhythm Stage Setup v4,jefftk,6mo
2337,Algorithmic Measure of Emergence v2.0,interstice,6mo
2338,"Infohazards, hacking, and Bricking—how to formalize these concepts?Q",Yitz,6mo
2339,Treat Examples as World-Building,adamShimi,6mo
2340,Best LessWrong posts for understanding scientific studies.Q,Sean McAneny,6mo
2341,I left Russia on March 8,avturchin,6mo
2342,What’s the chance a smart London resident dies of a Russian nuke in the next month?,DanielFilan,6mo
2343,What's the best ratio for Africans to starve compared to Ukrainians not dying in the war?Q,ChristianKl,6mo
2344,Ukraine Post #2: Options,Zvi,6mo
2345,Covid 3/10/22: We Have a Plan,Zvi,6mo
2346,"Donations, The First Year",pixx,7mo
